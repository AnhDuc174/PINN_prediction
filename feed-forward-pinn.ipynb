{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13449197,"sourceType":"datasetVersion","datasetId":8536862}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport math\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:18.789272Z","iopub.execute_input":"2025-10-21T12:39:18.789688Z","iopub.status.idle":"2025-10-21T12:39:18.794348Z","shell.execute_reply.started":"2025-10-21T12:39:18.789663Z","shell.execute_reply":"2025-10-21T12:39:18.793379Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"nu = 5e-4            \nrho = 1\nU_inlet = 1.0\n\ncyl_center = (0.5, 0.5)\ncyl_radius = 0.05\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:18.795436Z","iopub.execute_input":"2025-10-21T12:39:18.795836Z","iopub.status.idle":"2025-10-21T12:39:18.824310Z","shell.execute_reply.started":"2025-10-21T12:39:18.795808Z","shell.execute_reply":"2025-10-21T12:39:18.823348Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"class PINN(nn.Module):\n    def __init__(self, layers=[3] + [50]*20 + [3]):\n        super().__init__()\n        self.net = nn.ModuleList()\n        for i in range(len(layers)-1):\n            self.net.append(nn.Linear(layers[i], layers[i+1]))\n        self.activation = nn.Tanh()\n        # Xavier init\n        for m in self.net:\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        h = x\n        for i, layer in enumerate(self.net):\n            h = layer(h)\n            if i != len(self.net)-1:\n                h = self.activation(h)\n        return h  # [u,v,p]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:18.825911Z","iopub.execute_input":"2025-10-21T12:39:18.826561Z","iopub.status.idle":"2025-10-21T12:39:18.841496Z","shell.execute_reply.started":"2025-10-21T12:39:18.826540Z","shell.execute_reply":"2025-10-21T12:39:18.840504Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"data_dir = \"/kaggle/input/cfd-flow-pass-a-cylinder-0-01\"\nt_start = 0\nt_end = 501\ndt = 0.01\n\nxyt_list = []\nuvp_list = []\n\nfor i in range(t_start, t_end):\n    csv_path = os.path.join(data_dir, f\"result_{i}.csv\")\n    df = pd.read_csv(csv_path)\n\n    # timestep\n    t_val = i * dt\n    t_column = np.full_like(df[\"Points:0\"].values, fill_value=t_val, dtype=np.float32)\n\n    # (x, y, t)\n    xyt = np.stack([\n        df[\"Points:0\"].values,\n        df[\"Points:1\"].values,\n        t_column\n    ], axis=1)  \n\n    # (u, v, p)\n    uvp = np.stack([\n        df[\"u:0\"].values,\n        df[\"u:1\"].values,\n        df[\"p\"].values\n    ], axis=1)  \n\n    xyt_list.append(xyt)\n    uvp_list.append(uvp)\n\nxyt_tensor = torch.tensor(np.concatenate(xyt_list, axis=0), dtype=torch.float32)\nuvp_tensor = torch.tensor(np.concatenate(uvp_list, axis=0), dtype=torch.float32)\n\nprint(xyt_tensor.shape)\nprint(uvp_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:18.842420Z","iopub.execute_input":"2025-10-21T12:39:18.842684Z","iopub.status.idle":"2025-10-21T12:39:35.962098Z","shell.execute_reply.started":"2025-10-21T12:39:18.842662Z","shell.execute_reply":"2025-10-21T12:39:35.961252Z"}},"outputs":[{"name":"stdout","text":"torch.Size([20806530, 3])\ntorch.Size([20806530, 3])\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"time_col = xyt_tensor[:, 2]\n\n# Mask for t = 0\nmask_ic = (time_col == 0.0)\n\n# Apply mask\nxyt_tensor_ic = xyt_tensor[mask_ic]\nuvp_tensor_ic = uvp_tensor[mask_ic]\n\nprint(xyt_tensor_ic.shape)\nprint(uvp_tensor_ic.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:35.963618Z","iopub.execute_input":"2025-10-21T12:39:35.963850Z","iopub.status.idle":"2025-10-21T12:39:36.030378Z","shell.execute_reply.started":"2025-10-21T12:39:35.963832Z","shell.execute_reply":"2025-10-21T12:39:36.029691Z"}},"outputs":[{"name":"stdout","text":"torch.Size([41530, 3])\ntorch.Size([41530, 3])\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"def sample_points(inputs,\n                  outputs,\n                  N_per_timestep,\n                  seed=None,\n                  device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    obstacle_center=(0.5, 0.5)\n    obstacle_radius=0.05\n\n    # Refinement bounds and percents\n    refine_1 = ((0.35, 0.65), (0.35, 0.65))\n    percent_1 = 45\n\n    refine_2 = ((0.65, 2.5), (0.35, 0.65))\n    percent_2 = 35\n\n    percent_3 = 100 - (percent_1 + percent_2)\n\n    # Convert to numpy\n    pts_np = inputs.cpu().numpy()\n    uvp_np = outputs.cpu().numpy()\n\n    # Mask out obstacle region\n    dx = pts_np[:, 0] - obstacle_center[0]\n    dy = pts_np[:, 1] - obstacle_center[1]\n    mask = dx**2 + dy**2 >= obstacle_radius**2\n    pts_np = pts_np[mask]\n    uvp_np = uvp_np[mask]\n\n    # Timesteps\n    timesteps = np.unique(pts_np[:, 2])\n    timesteps = np.random.choice(timesteps, size=20, replace=False)\n\n    pts_final_list, uvp_final_list = [], []\n\n    for t in timesteps:\n        mask_t = np.abs(pts_np[:, 2] - t) < 1e-12\n        pts_t, uvp_t = pts_np[mask_t], uvp_np[mask_t]\n\n        # --- region 1\n        mask_r1 = (\n            (pts_t[:, 0] >= refine_1[0][0]) & (pts_t[:, 0] <= refine_1[0][1]) &\n            (pts_t[:, 1] >= refine_1[1][0]) & (pts_t[:, 1] <= refine_1[1][1])\n        )\n        pts_r1, uvp_r1 = pts_t[mask_r1], uvp_t[mask_r1]\n\n        # --- region 2\n        mask_r2 = (\n            (pts_t[:, 0] >= refine_2[0][0]) & (pts_t[:, 0] <= refine_2[0][1]) &\n            (pts_t[:, 1] >= refine_2[1][0]) & (pts_t[:, 1] <= refine_2[1][1])\n        )\n        pts_r2, uvp_r2 = pts_t[mask_r2], uvp_t[mask_r2]\n\n        # --- region 3 = rest\n        mask_r3 = ~(mask_r1 | mask_r2)\n        pts_r3, uvp_r3 = pts_t[mask_r3], uvp_t[mask_r3]\n\n        # How many per region\n        N1 = int(N_per_timestep * percent_1 / 100.0)\n        N2 = int(N_per_timestep * percent_2 / 100.0)\n        N3 = N_per_timestep - N1 - N2\n\n        def sample_region(pts_region, uvp_region, N):\n            if len(pts_region) == 0:  # if no points, fallback to random global\n                return np.empty((0, 3)), np.empty((0, 3))\n            if len(pts_region) < N:\n                idx = np.random.choice(len(pts_region), size=N, replace=True)\n            else:\n                idx = np.random.choice(len(pts_region), size=N, replace=False)\n            return pts_region[idx], uvp_region[idx]\n\n        # Sample each region\n        sp1, uv1 = sample_region(pts_r1, uvp_r1, N1)\n        sp2, uv2 = sample_region(pts_r2, uvp_r2, N2)\n        sp3, uv3 = sample_region(pts_r3, uvp_r3, N3)\n\n        pts_final_list.append(np.vstack([sp1, sp2, sp3]))\n        uvp_final_list.append(np.vstack([uv1, uv2, uv3]))\n\n    # Concatenate\n    pts_final = np.vstack(pts_final_list)\n    uvp_final = np.vstack(uvp_final_list)\n\n    # Shuffle\n    perm = np.random.permutation(len(pts_final))\n    pts_final = pts_final[perm]\n    uvp_final = uvp_final[perm]\n\n    # Torch tensors\n    points = torch.tensor(pts_final, dtype=torch.float32, device=device)\n    uvp = torch.tensor(uvp_final, dtype=torch.float32, device=device)\n    u = uvp[:, 0:1]\n    v = uvp[:, 1:2]\n    p = uvp[:, 2:3]\n\n    return points, u, v, p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.031063Z","iopub.execute_input":"2025-10-21T12:39:36.031556Z","iopub.status.idle":"2025-10-21T12:39:36.043897Z","shell.execute_reply.started":"2025-10-21T12:39:36.031536Z","shell.execute_reply":"2025-10-21T12:39:36.043177Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef sample_collocation_points(N_per_timestep,\n                              seed=None,\n                              device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    domain_bound = ((0, 2.5), (0, 1))\n    obstacle_center = (0.5, 0.5)\n    obstacle_radius = 0.05\n\n    # --- Refinement regions ---\n    refine_1 = ((0.35, 0.65), (0.35, 0.65))\n    percent_refine_1 = 45\n\n    refine_2 = ((0.65, 2.5), (0.35, 0.65))\n    percent_refine_2 = 35\n\n    percent_refine_3 = 100 - (percent_refine_1 + percent_refine_2)\n\n    all_timesteps = np.arange(0, total_time + dt, dt)\n\n    timesteps = np.random.choice(all_timesteps, size=20, replace=False)\n    \n    all_points = []\n\n    def in_region(x, y, bounds):\n        (x_min, x_max), (y_min, y_max) = bounds\n        return (x >= x_min) & (x <= x_max) & (y >= y_min) & (y <= y_max)\n\n    for t in timesteps:\n        pts_list = []\n\n        # --- Refine 1\n        N1 = int(N_per_timestep * percent_refine_1 / 100.0)\n        (x_min, x_max), (y_min, y_max) = refine_1\n        x = np.random.uniform(x_min, x_max, N1)\n        y = np.random.uniform(y_min, y_max, N1)\n        t_vals = np.full(N1, t)\n        pts_list.append(np.stack([x, y, t_vals], axis=1))\n\n        # --- Refine 2\n        N2 = int(N_per_timestep * percent_refine_2 / 100.0)\n        (x_min, x_max), (y_min, y_max) = refine_2\n        x = np.random.uniform(x_min, x_max, N2)\n        y = np.random.uniform(y_min, y_max, N2)\n        t_vals = np.full(N2, t)\n        pts_list.append(np.stack([x, y, t_vals], axis=1))\n\n        # --- Refine 3 (domain minus refine_1 and refine_2)\n        N3 = N_per_timestep - (N1 + N2)\n        pts_ref3 = []\n        while len(pts_ref3) < N3:\n            n_more = N3 - len(pts_ref3)\n            x = np.random.uniform(domain_bound[0][0], domain_bound[0][1], n_more)\n            y = np.random.uniform(domain_bound[1][0], domain_bound[1][1], n_more)\n\n            mask_not_r1 = ~in_region(x, y, refine_1)\n            mask_not_r2 = ~in_region(x, y, refine_2)\n            mask = mask_not_r1 & mask_not_r2\n\n            x, y = x[mask], y[mask]\n            t_vals = np.full(len(x), t)\n            pts_ref3.append(np.stack([x, y, t_vals], axis=1))\n\n        pts_list.append(np.vstack(pts_ref3))\n\n        # Combine\n        pts_np = np.vstack(pts_list)\n\n        # --- Mask out obstacle ---\n        dx = pts_np[:, 0] - obstacle_center[0]\n        dy = pts_np[:, 1] - obstacle_center[1]\n        mask = dx**2 + dy**2 >= obstacle_radius**2\n        pts_np = pts_np[mask]\n\n        # Resample if too few\n        while len(pts_np) < N_per_timestep:\n            n_more = N_per_timestep - len(pts_np)\n            x = np.random.uniform(refine_2[0][0], refine_2[0][1], n_more)\n            y = np.random.uniform(refine_2[1][0], refine_2[1][1], n_more)\n            t_vals = np.full(n_more, t)\n            new_pts = np.stack([x, y, t_vals], axis=1)\n\n            dx = new_pts[:, 0] - obstacle_center[0]\n            dy = new_pts[:, 1] - obstacle_center[1]\n            mask = dx**2 + dy**2 >= obstacle_radius**2\n            new_pts = new_pts[mask]\n\n            pts_np = np.vstack([pts_np, new_pts])\n\n        pts_np = pts_np[:N_per_timestep]\n        all_points.append(pts_np)\n\n    # Concatenate and shuffle\n    all_points = np.vstack(all_points)\n    np.random.shuffle(all_points)\n\n    return torch.tensor(all_points, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.044779Z","iopub.execute_input":"2025-10-21T12:39:36.045066Z","iopub.status.idle":"2025-10-21T12:39:36.062468Z","shell.execute_reply.started":"2025-10-21T12:39:36.045028Z","shell.execute_reply":"2025-10-21T12:39:36.061760Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef sample_collocation_points(N_per_timestep,\n                                   seed=None,\n                                   device=\"cuda\"):\n\n\n    total_time=5.0\n    dt=0.01\n    n_timesteps_per_iter=20\n\n    \n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # --- Domain and geometry ---\n    domain_x, domain_y = (0, 2.5), (0, 1)\n    obstacle_center = (0.5, 0.5)\n    obstacle_radius2 = 0.05 ** 2\n\n    # --- Refinement regions ---\n    refine_1 = ((0.35, 0.65), (0.35, 0.65))\n    refine_2 = ((0.65, 2.5), (0.35, 0.65))\n    w1, w2 = 0.40, 0.35   # weights\n    w3 = 1.0 - (w1 + w2)\n\n    timesteps = np.arange(0, total_time + dt, dt)\n    chosen_timesteps = np.random.choice(timesteps, size=n_timesteps_per_iter, replace=False)\n\n    # --- Vectorized sampling ---\n    # Total number of samples overall\n    total_pts = N_per_timestep * n_timesteps_per_iter\n\n    # Assign which region each point belongs to\n    regions = np.random.choice(3, size=total_pts, p=[w1, w2, w3])\n\n    # Preallocate\n    x = np.empty(total_pts, dtype=np.float32)\n    y = np.empty(total_pts, dtype=np.float32)\n    t = np.repeat(chosen_timesteps, N_per_timestep).astype(np.float32)\n\n    # Region 1\n    mask1 = regions == 0\n    n1 = mask1.sum()\n    x[mask1] = np.random.uniform(*refine_1[0], n1)\n    y[mask1] = np.random.uniform(*refine_1[1], n1)\n\n    # Region 2\n    mask2 = regions == 1\n    n2 = mask2.sum()\n    x[mask2] = np.random.uniform(*refine_2[0], n2)\n    y[mask2] = np.random.uniform(*refine_2[1], n2)\n\n    # Region 3 (rest of domain)\n    mask3 = regions == 2\n    n3 = mask3.sum()\n    x[mask3] = np.random.uniform(*domain_x, n3)\n    y[mask3] = np.random.uniform(*domain_y, n3)\n\n    # --- Remove obstacle points (vectorized) ---\n    dx = x - obstacle_center[0]\n    dy = y - obstacle_center[1]\n    mask_keep = dx**2 + dy**2 >= obstacle_radius2\n\n    x, y, t = x[mask_keep], y[mask_keep], t[mask_keep]\n\n    # If too few points (after obstacle removal), resample fast (no loops)\n    n_needed = total_pts - len(x)\n    if n_needed > 0:\n        xr = np.random.uniform(*refine_2[0], n_needed)\n        yr = np.random.uniform(*refine_2[1], n_needed)\n        tr = np.random.choice(chosen_timesteps, size=n_needed)\n        dxr = xr - obstacle_center[0]\n        dyr = yr - obstacle_center[1]\n        keep = dxr**2 + dyr**2 >= obstacle_radius2\n        x = np.concatenate([x, xr[keep]])\n        y = np.concatenate([y, yr[keep]])\n        t = np.concatenate([t, tr[keep]])\n        x, y, t = x[:total_pts], y[:total_pts], t[:total_pts]\n\n    pts_np = np.stack([x, y, t], axis=1)\n    np.random.shuffle(pts_np)\n\n    return torch.tensor(pts_np, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.063332Z","iopub.execute_input":"2025-10-21T12:39:36.063543Z","iopub.status.idle":"2025-10-21T12:39:36.085525Z","shell.execute_reply.started":"2025-10-21T12:39:36.063494Z","shell.execute_reply":"2025-10-21T12:39:36.084746Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"def sample_inlet(N_per_timestep,\n                 seed=None,\n                 device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    y_bounds = (0, 1)\n    x_in = 0.0\n\n    timesteps = np.arange(0, total_time + dt, dt)  # all time steps\n\n    all_points = []\n\n    for t in timesteps:\n        y = np.random.uniform(y_bounds[0], y_bounds[1], N_per_timestep)\n        x = np.full(N_per_timestep, x_in)\n        t_vals = np.full(N_per_timestep, t)\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.086376Z","iopub.execute_input":"2025-10-21T12:39:36.086871Z","iopub.status.idle":"2025-10-21T12:39:36.103344Z","shell.execute_reply.started":"2025-10-21T12:39:36.086829Z","shell.execute_reply":"2025-10-21T12:39:36.102758Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def sample_cylinder_surface(N_per_timestep,\n                            seed=None,\n                            device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    cyl_center = (0.5, 0.5)\n    cyl_radius = 0.05\n\n    timesteps = np.arange(0, total_time + dt, dt)\n    all_points = []\n    cx, cy = cyl_center\n\n    for t in timesteps:\n        # Sample angles uniformly around the circle\n        theta = np.random.uniform(0, 2*np.pi, N_per_timestep)\n        x = cx + cyl_radius * np.cos(theta)\n        y = cy + cyl_radius * np.sin(theta)\n        t_vals = np.full(N_per_timestep, t)\n\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.103956Z","iopub.execute_input":"2025-10-21T12:39:36.104172Z","iopub.status.idle":"2025-10-21T12:39:36.116419Z","shell.execute_reply.started":"2025-10-21T12:39:36.104148Z","shell.execute_reply":"2025-10-21T12:39:36.115678Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"def sample_top_bottom(N_per_timestep,\n                      seed=None,\n                      device=\"cpu\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    x_bounds = (0, 2.5)\n    y_top = 1.0\n    y_bot = 0.0\n\n    timesteps = np.arange(0, total_time + dt, dt)\n\n    # Split evenly between top and bottom\n    N_top = N_per_timestep // 2\n    N_bot = N_per_timestep - N_top  # handle odd numbers\n\n    all_points = []\n\n    for t in timesteps:\n        # Top wall\n        x_top = np.random.uniform(x_bounds[0], x_bounds[1], N_top)\n        y_top_arr = np.full(N_top, y_top)\n        t_top = np.full(N_top, t)\n        pts_top = np.stack([x_top, y_top_arr, t_top], axis=1)\n\n        # Bottom wall\n        x_bot = np.random.uniform(x_bounds[0], x_bounds[1], N_bot)\n        y_bot_arr = np.full(N_bot, y_bot)\n        t_bot = np.full(N_bot, t)\n        pts_bot = np.stack([x_bot, y_bot_arr, t_bot], axis=1)\n\n        all_points.append(pts_top)\n        all_points.append(pts_bot)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.118759Z","iopub.execute_input":"2025-10-21T12:39:36.119176Z","iopub.status.idle":"2025-10-21T12:39:36.132347Z","shell.execute_reply.started":"2025-10-21T12:39:36.119157Z","shell.execute_reply":"2025-10-21T12:39:36.131745Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"def sample_initial(N,\n                   xyt_tensor_ic, \n                   uvp_tensor_ic,\n                   device=\"cuda\"):\n\n\n    # Cylinder info\n    cyl_center = (0.5, 0.5)\n    cyl_radius = 0.05\n\n    # Hardcoded bounds\n    bound1 = ((0.35, 0.65), (0.35, 0.65))  # region 1 (around cylinder)\n    bound2 = ((0.65, 2.5), (0.35, 0.65))   # region 2 (wake)\n    \n    # Hardcoded percentages\n    perc_r1, perc_r2, perc_r3 = 0.5, 0.3, 0.2\n\n    N_r1 = int(N * perc_r1)\n    N_r2 = int(N * perc_r2)\n    N_r3 = N - N_r1 - N_r2  # ensure exact total\n\n    # Unpack coordinates\n    x = xyt_tensor_ic[:, 0]\n    y = xyt_tensor_ic[:, 1]\n\n    # Mask out cylinder\n    cx, cy = cyl_center\n    dx, dy = x - cx, y - cy\n    mask_cyl = (dx**2 + dy**2) >= cyl_radius**2\n    xyt_valid = xyt_tensor_ic[mask_cyl]\n    uvp_valid = uvp_tensor_ic[mask_cyl]\n\n    # --- Region 1 (refinement box around cylinder) ---\n    mask_r1 = ((xyt_valid[:, 0] >= bound1[0][0]) & (xyt_valid[:, 0] <= bound1[0][1]) &\n               (xyt_valid[:, 1] >= bound1[1][0]) & (xyt_valid[:, 1] <= bound1[1][1]))\n\n    # --- Region 2 (wake region) ---\n    mask_r2 = ((xyt_valid[:, 0] >= bound2[0][0]) & (xyt_valid[:, 0] <= bound2[0][1]) &\n               (xyt_valid[:, 1] >= bound2[1][0]) & (xyt_valid[:, 1] <= bound2[1][1]))\n\n    # --- Region 3 = rest ---\n    mask_r3 = ~(mask_r1 | mask_r2)\n\n    # Helper: sample points from region\n    def sample_region(mask, N):\n        xyt_reg = xyt_valid[mask]\n        uvp_reg = uvp_valid[mask]\n        if len(xyt_reg) == 0:\n            return None, None\n        replace = xyt_reg.shape[0] < N\n        idx = torch.randint(0, xyt_reg.shape[0], (N,), device=\"cpu\", dtype=torch.long) if replace else \\\n                torch.randperm(xyt_reg.shape[0])[:N]\n\n        return xyt_reg[idx].to(device), uvp_reg[idx].to(device)\n\n    # Sample each region\n    xyt_r1, uvp_r1 = sample_region(mask_r1, N_r1)\n    xyt_r2, uvp_r2 = sample_region(mask_r2, N_r2)\n    xyt_r3, uvp_r3 = sample_region(mask_r3, N_r3)\n\n    # Concatenate\n    sampled_xyt = torch.cat([t for t in [xyt_r1, xyt_r2, xyt_r3] if t is not None], dim=0)\n    sampled_uvp = torch.cat([t for t in [uvp_r1, uvp_r2, uvp_r3] if t is not None], dim=0)\n\n    return sampled_xyt, sampled_uvp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.133033Z","iopub.execute_input":"2025-10-21T12:39:36.133194Z","iopub.status.idle":"2025-10-21T12:39:36.147178Z","shell.execute_reply.started":"2025-10-21T12:39:36.133181Z","shell.execute_reply":"2025-10-21T12:39:36.146474Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def compute_residuals(model, X):\n\n    nu = 5e-4            \n    rho = 1\n\n    X = X.clone().detach().requires_grad_(True)\n    out = model(X)\n    u = out[:,0:1]\n    v = out[:,1:2]\n    p = out[:,2:3]\n\n    grads_u = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    u_x = grads_u[:,0:1]\n    u_y = grads_u[:,1:2]\n    u_t = grads_u[:,2:3]\n\n    grads_v = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n    v_x = grads_v[:,0:1]\n    v_y = grads_v[:,1:2]\n    v_t = grads_v[:,2:3]\n\n    grads_p = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n    p_x = grads_p[:,0:1]\n    p_y = grads_p[:,1:2]\n\n    # second derivatives\n    grads_ux = torch.autograd.grad(grads_u, X, torch.ones_like(grads_u), create_graph=True)[0]\n    grads_vx = torch.autograd.grad(grads_v, X, torch.ones_like(grads_v), create_graph=True)[0]\n    u_xx, u_yy = grads_ux[:,0:1], grads_ux[:,1:2]\n    v_xx, v_yy = grads_vx[:,0:1], grads_vx[:,1:2]\n\n    # continuity\n    cont = u_x + v_y\n\n    # momentum equations\n    mom_u = u_t + (u * u_x + v * u_y) + (1/rho) * p_x - nu * (u_xx + u_yy)\n    mom_v = v_t + (u * v_x + v * v_y) + (1/rho) * p_y - nu * (v_xx + v_yy)\n\n\n    return cont, mom_u, mom_v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.147970Z","iopub.execute_input":"2025-10-21T12:39:36.148199Z","iopub.status.idle":"2025-10-21T12:39:36.163671Z","shell.execute_reply.started":"2025-10-21T12:39:36.148175Z","shell.execute_reply":"2025-10-21T12:39:36.162915Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"def data_loss(model, pts, u, v, p, device):\n\n    # Model prediction\n    pred = model(pts)\n    u_pred, v_pred, p_pred = pred[:,0:1], pred[:,1:2], pred[:,2:3]\n\n    mse = nn.MSELoss()\n    return mse(u_pred, u) + mse(v_pred, v) + mse(p_pred, p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.164391Z","iopub.execute_input":"2025-10-21T12:39:36.164630Z","iopub.status.idle":"2025-10-21T12:39:36.178379Z","shell.execute_reply.started":"2025-10-21T12:39:36.164614Z","shell.execute_reply":"2025-10-21T12:39:36.177840Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"os.makedirs('models', exist_ok=True)\nNf, Nf_data, Nic, Nd, Nn, Ni = 30000, 10000, 12000, 200, 700, 150","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.179091Z","iopub.execute_input":"2025-10-21T12:39:36.179270Z","iopub.status.idle":"2025-10-21T12:39:36.189779Z","shell.execute_reply.started":"2025-10-21T12:39:36.179249Z","shell.execute_reply":"2025-10-21T12:39:36.189169Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"def compute_residuals_in_batches(model, X, batch_size=10000):\n    \"\"\"\n    Compute PDE residuals in mini-batches to avoid CUDA OOM.\n    \"\"\"\n    cont_list, mu_list, mv_list = [], [], []\n    n = X.size(0)\n\n    for i in range(0, n, batch_size):\n        X_batch = X[i:i+batch_size].clone().detach().requires_grad_(True)\n        cont, mu, mv = compute_residuals(model, X_batch)\n\n        cont_list.append(cont.detach())\n        mu_list.append(mu.detach())\n        mv_list.append(mv.detach())\n\n        # free up memory\n        del cont, mu, mv, X_batch\n        torch.cuda.empty_cache()\n\n    # concatenate results back\n    return (torch.cat(cont_list, dim=0),\n            torch.cat(mu_list, dim=0),\n            torch.cat(mv_list, dim=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.190565Z","iopub.execute_input":"2025-10-21T12:39:36.190869Z","iopub.status.idle":"2025-10-21T12:39:36.202620Z","shell.execute_reply.started":"2025-10-21T12:39:36.190852Z","shell.execute_reply":"2025-10-21T12:39:36.202102Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\n\nmse_loss = nn.MSELoss()\n\ndef train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=40000, print_every=2000, save_every=2000,\n               λ_data=1.0, alpha_pde=1.0, seed=None, device=\"cuda\",\n               checkpoint_dir=\"models\"):\n\n    print(\"Start training loop\")\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    start_iter = 0\n\n    # --- Try to resume from the latest checkpoint ---\n    latest_ckpt = None\n    if os.path.exists(checkpoint_dir):\n        ckpts = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"pinn_checkpoint_\")]\n        if ckpts:\n            latest_ckpt = max(ckpts, key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0]))\n\n    if latest_ckpt:\n        path = os.path.join(checkpoint_dir, latest_ckpt)\n        print(f\"Loading checkpoint from {path}...\")\n        checkpoint = torch.load(path, map_location=device)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        start_iter = checkpoint[\"iter\"] + 1\n        print(f\"Resuming training from iteration {start_iter}\")\n\n\n    for it in range(start_iter, num_iters):\n\n        #print(\"Start collocation sampling\")\n        # Physics residual loss\n        X_f = sample_collocation_points(N_per_timestep=Nf,\n                                          seed=None,\n                                          device=\"cuda\")\n        #print(\"End collocation sampling\")\n        \n        #print(\"Start data sampling\")\n        X_f_data, u, v, p = sample_points(inputs=xyt_tensor,\n                                      outputs=uvp_tensor,\n                                      N_per_timestep=Nf_data,\n                                      seed=None,\n                                      device=\"cuda\")\n        #print(\"End data sampling\")\n\n        #print(\"Start calculate pde\")\n        cont, mu, mv = compute_residuals_in_batches(model, X_f, batch_size=10000)\n        \n        loss_f = (mse_loss(cont, torch.zeros_like(cont)) +\n                  mse_loss(mu, torch.zeros_like(mu)) +\n                  mse_loss(mv, torch.zeros_like(mv)))\n        #print(\"End calculate pde\")\n\n\n        # Data loss\n        loss_data = data_loss(model, X_f_data, u, v, p, device=\"cuda\")\n\n\n        # Initial condition\n        X_ic, Y_ic = sample_initial(N=Nic,\n                                   xyt_tensor_ic=xyt_tensor_ic, \n                                   uvp_tensor_ic=uvp_tensor_ic,\n                                   device=\"cuda\")\n\n        \n        out_ic = model(X_ic)\n        loss_ic = (mse_loss(out_ic[:, 0:1], Y_ic[:, 0:1]) +\n                   mse_loss(out_ic[:, 1:2], Y_ic[:, 1:2]) +\n                   mse_loss(out_ic[:, 2:3], Y_ic[:, 2:3]))\n\n\n        # Inlet condition\n        X_in = sample_inlet(N_per_timestep=Ni,\n                            seed=None,\n                            device=\"cuda\")\n\n        \n        out_in = model(X_in)\n        loss_in = (mse_loss(out_in[:, 0:1], torch.ones_like(out_in[:, 0:1])) +\n                   mse_loss(out_in[:, 1:2], torch.zeros_like(out_in[:, 1:2])))\n\n\n        # Cylinder surface\n        X_cyl = sample_cylinder_surface(N_per_timestep=Nd,\n                                        seed=None,\n                                        device=\"cuda\")\n\n        \n        out_cyl = model(X_cyl)\n        loss_cyl = (mse_loss(out_cyl[:, 0:1], torch.zeros_like(out_cyl[:, 0:1])) +\n                    mse_loss(out_cyl[:, 1:2], torch.zeros_like(out_cyl[:, 1:2])))\n\n\n        # Top/bottom boundaries\n        X_tb = sample_top_bottom(N_per_timestep=Nn,\n                                seed=None,\n                                device=\"cuda\")\n\n        out_tb = model(X_tb)\n        loss_tb = (mse_loss(out_tb[:, 1:2], torch.zeros_like(out_tb[:, 1:2])))\n\n\n        # Total loss\n        loss = alpha_pde * loss_f + λ_data * loss_data + loss_ic + loss_in + loss_cyl + loss_tb\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if it % print_every == 0:\n            print(f\"Adam Iter {it}/{num_iters} - Total loss {loss.item():.6f}\\n\"\n                  f\"Data {loss_data.item():.6f} | PDE {loss_f.item():.4e}\\n\"\n                  f\"IC {loss_ic.item():.4e}, Inlet {loss_in.item():.4e}\\n\"\n                  f\"Cylinder {loss_cyl.item():.4e}, TopBottom {loss_tb.item():.4e}\")\n\n         # --- Save checkpoint every save_every iterations ---\n        if (it + 1) % save_every == 0:\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            ckpt_path = os.path.join(checkpoint_dir, f\"pinn_checkpoint_{it+1}.pth\")\n            torch.save({\n                \"iter\": it,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict()\n            }, ckpt_path)\n            print(f\"Checkpoint saved → {ckpt_path}\")\n\n\n    # Final model save\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    final_path = os.path.join(checkpoint_dir, \"pinn_ns_adam_final.pth\")\n    torch.save(model.state_dict(), final_path)\n    print(f\"Final model saved at {final_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.203337Z","iopub.execute_input":"2025-10-21T12:39:36.203504Z","iopub.status.idle":"2025-10-21T12:39:36.226479Z","shell.execute_reply.started":"2025-10-21T12:39:36.203490Z","shell.execute_reply":"2025-10-21T12:39:36.225749Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = PINN().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.227321Z","iopub.execute_input":"2025-10-21T12:39:36.227619Z","iopub.status.idle":"2025-10-21T12:39:36.247373Z","shell.execute_reply.started":"2025-10-21T12:39:36.227579Z","shell.execute_reply":"2025-10-21T12:39:36.246767Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=3000, print_every=100, save_every=100,\n               λ_data=1.0, alpha_pde=1.0, seed=None, device=\"cuda\",\n               checkpoint_dir=\"models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:39:36.248087Z","iopub.execute_input":"2025-10-21T12:39:36.248362Z","iopub.status.idle":"2025-10-21T17:58:35.073297Z","shell.execute_reply.started":"2025-10-21T12:39:36.248335Z","shell.execute_reply":"2025-10-21T17:58:35.072272Z"}},"outputs":[{"name":"stdout","text":"Start training loop\nAdam Iter 0/3000 - Total loss 5.659728\nData 1.732247 | PDE 2.9576e-02\nIC 2.3366e+00, Inlet 1.3219e+00\nCylinder 1.2090e-01, TopBottom 1.1850e-01\nCheckpoint saved → models/pinn_checkpoint_100.pth\nAdam Iter 100/3000 - Total loss 0.955230\nData 0.295785 | PDE 4.1311e-02\nIC 1.1248e-01, Inlet 9.7363e-02\nCylinder 4.0821e-01, TopBottom 8.2529e-05\nCheckpoint saved → models/pinn_checkpoint_200.pth\nAdam Iter 200/3000 - Total loss 7.302191\nData 0.333959 | PDE 6.7151e+00\nIC 1.2770e-01, Inlet 1.0308e-02\nCylinder 1.1472e-01, TopBottom 4.2002e-04\nCheckpoint saved → models/pinn_checkpoint_300.pth\nAdam Iter 300/3000 - Total loss 18.130033\nData 0.316262 | PDE 1.7639e+01\nIC 9.4062e-02, Inlet 4.2192e-04\nCylinder 8.0218e-02, TopBottom 3.2447e-04\nCheckpoint saved → models/pinn_checkpoint_400.pth\nAdam Iter 400/3000 - Total loss 19.078127\nData 0.317021 | PDE 1.8609e+01\nIC 8.4733e-02, Inlet 5.1193e-04\nCylinder 6.6175e-02, TopBottom 4.8988e-04\nCheckpoint saved → models/pinn_checkpoint_500.pth\nAdam Iter 500/3000 - Total loss 21.303234\nData 0.323906 | PDE 2.0836e+01\nIC 9.0495e-02, Inlet 4.2576e-04\nCylinder 5.2394e-02, TopBottom 5.1219e-04\nCheckpoint saved → models/pinn_checkpoint_600.pth\nAdam Iter 600/3000 - Total loss 29.478546\nData 0.303153 | PDE 2.9032e+01\nIC 8.2660e-02, Inlet 3.7739e-03\nCylinder 5.6654e-02, TopBottom 3.6326e-04\nCheckpoint saved → models/pinn_checkpoint_700.pth\nAdam Iter 700/3000 - Total loss 30.900261\nData 0.296696 | PDE 3.0447e+01\nIC 7.4455e-02, Inlet 2.2707e-03\nCylinder 7.9430e-02, TopBottom 6.4356e-04\nCheckpoint saved → models/pinn_checkpoint_800.pth\nAdam Iter 800/3000 - Total loss 55.407490\nData 0.386732 | PDE 5.4845e+01\nIC 1.0381e-01, Inlet 8.4567e-04\nCylinder 7.0017e-02, TopBottom 9.5039e-04\nCheckpoint saved → models/pinn_checkpoint_900.pth\nAdam Iter 900/3000 - Total loss 28.353565\nData 0.312930 | PDE 2.7872e+01\nIC 8.9125e-02, Inlet 2.3847e-03\nCylinder 7.6210e-02, TopBottom 6.6134e-04\nCheckpoint saved → models/pinn_checkpoint_1000.pth\nAdam Iter 1000/3000 - Total loss 31.929560\nData 0.300042 | PDE 3.1485e+01\nIC 8.2766e-02, Inlet 2.6593e-03\nCylinder 5.8690e-02, TopBottom 7.2393e-04\nCheckpoint saved → models/pinn_checkpoint_1100.pth\nAdam Iter 1100/3000 - Total loss 41.208385\nData 0.278912 | PDE 4.0833e+01\nIC 6.1568e-02, Inlet 3.3210e-03\nCylinder 2.9909e-02, TopBottom 1.4101e-03\nCheckpoint saved → models/pinn_checkpoint_1200.pth\nAdam Iter 1200/3000 - Total loss 13.231677\nData 0.177020 | PDE 1.2886e+01\nIC 4.8918e-02, Inlet 8.4489e-04\nCylinder 1.1782e-01, TopBottom 6.5752e-04\nCheckpoint saved → models/pinn_checkpoint_1300.pth\nAdam Iter 1300/3000 - Total loss 13.733541\nData 0.086598 | PDE 1.3611e+01\nIC 2.4591e-02, Inlet 2.8431e-04\nCylinder 1.0255e-02, TopBottom 3.5769e-04\nCheckpoint saved → models/pinn_checkpoint_1400.pth\nAdam Iter 1400/3000 - Total loss 16.823166\nData 0.114981 | PDE 1.6650e+01\nIC 3.1579e-02, Inlet 1.5286e-03\nCylinder 2.4857e-02, TopBottom 5.2568e-04\nCheckpoint saved → models/pinn_checkpoint_1500.pth\nAdam Iter 1500/3000 - Total loss 14.972694\nData 0.092235 | PDE 1.4831e+01\nIC 4.0102e-02, Inlet 2.1648e-03\nCylinder 4.8690e-03, TopBottom 2.2569e-03\nCheckpoint saved → models/pinn_checkpoint_1600.pth\nAdam Iter 1600/3000 - Total loss 13.855310\nData 0.067977 | PDE 1.3771e+01\nIC 1.0891e-02, Inlet 2.2702e-04\nCylinder 5.3435e-03, TopBottom 1.4709e-04\nCheckpoint saved → models/pinn_checkpoint_1700.pth\nAdam Iter 1700/3000 - Total loss 14.754671\nData 0.068951 | PDE 1.4669e+01\nIC 1.0180e-02, Inlet 1.0789e-04\nCylinder 5.9913e-03, TopBottom 1.4026e-04\nCheckpoint saved → models/pinn_checkpoint_1800.pth\nAdam Iter 1800/3000 - Total loss 16.409693\nData 0.179742 | PDE 1.6157e+01\nIC 3.4837e-02, Inlet 1.4105e-03\nCylinder 3.6099e-02, TopBottom 7.4193e-04\nCheckpoint saved → models/pinn_checkpoint_1900.pth\nAdam Iter 1900/3000 - Total loss 13.024901\nData 0.052937 | PDE 1.2959e+01\nIC 7.0038e-03, Inlet 7.1782e-05\nCylinder 5.4537e-03, TopBottom 1.5642e-04\nCheckpoint saved → models/pinn_checkpoint_2000.pth\nAdam Iter 2000/3000 - Total loss 14.268999\nData 0.093421 | PDE 1.4140e+01\nIC 2.2467e-02, Inlet 7.1658e-04\nCylinder 1.0810e-02, TopBottom 1.6308e-03\nCheckpoint saved → models/pinn_checkpoint_2100.pth\nAdam Iter 2100/3000 - Total loss 11.582495\nData 0.055207 | PDE 1.1514e+01\nIC 6.0131e-03, Inlet 2.9412e-04\nCylinder 6.4142e-03, TopBottom 1.2620e-04\nCheckpoint saved → models/pinn_checkpoint_2200.pth\nAdam Iter 2200/3000 - Total loss 24.910561\nData 0.140856 | PDE 2.4728e+01\nIC 2.4919e-02, Inlet 1.4740e-03\nCylinder 1.2710e-02, TopBottom 2.1294e-03\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2731451239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                \u001b[0mλ_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_pde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                checkpoint_dir=\"models\")\n","\u001b[0;32m/tmp/ipykernel_36/931185162.py\u001b[0m in \u001b[0;36mtrain_adam\u001b[0;34m(model, Nf, Nf_data, Nic, Nd, Nn, Ni, num_iters, print_every, save_every, λ_data, alpha_pde, seed, device, checkpoint_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(\"Start data sampling\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         X_f_data, u, v, p = sample_points(inputs=xyt_tensor,\n\u001b[0m\u001b[1;32m     46\u001b[0m                                       \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muvp_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                       \u001b[0mN_per_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNf_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1992860642.py\u001b[0m in \u001b[0;36msample_points\u001b[0;34m(inputs, outputs, N_per_timestep, seed, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmask_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mpts_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muvp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muvp_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":76}]}