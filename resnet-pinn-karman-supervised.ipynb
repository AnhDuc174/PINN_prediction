{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13449197,"sourceType":"datasetVersion","datasetId":8536862}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport math\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:14.821279Z","iopub.execute_input":"2025-11-06T15:03:14.821461Z","iopub.status.idle":"2025-11-06T15:03:20.540523Z","shell.execute_reply.started":"2025-11-06T15:03:14.821444Z","shell.execute_reply":"2025-11-06T15:03:20.539755Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"nu = 5e-4            \nrho = 1\nU_inlet = 1.0\n\ncyl_center = (0.5, 0.5)\ncyl_radius = 0.05\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:20.542293Z","iopub.execute_input":"2025-11-06T15:03:20.542660Z","iopub.status.idle":"2025-11-06T15:03:20.607401Z","shell.execute_reply.started":"2025-11-06T15:03:20.542640Z","shell.execute_reply":"2025-11-06T15:03:20.606698Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass ResNetPINN(nn.Module):\n    def __init__(self, layers=[3] + [128] * 20 + [3]):\n        super().__init__()\n\n        # Input layer\n        self.input_layer = nn.Linear(layers[0], layers[1])\n\n        # Hidden layers (residual blocks)\n        self.hidden_layers = nn.ModuleList()\n        for i in range(1, len(layers) - 2):\n            self.hidden_layers.append(nn.Linear(layers[i], layers[i + 1]))\n\n        # Output layer\n        self.output_layer = nn.Linear(layers[-2], layers[-1])\n\n        # Activation function\n        self.activation = nn.Tanh()\n\n        # Xavier initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        # Input layer\n        h = self.activation(self.input_layer(x))\n\n        # Residual connections through hidden layers\n        for layer in self.hidden_layers:\n            h_in = h\n            h = self.activation(layer(h))\n\n            # Add residual connection (only if dimensions match)\n            if h.shape == h_in.shape:\n                h = h + h_in\n\n        # Output layer (no activation)\n        out = self.output_layer(h)\n\n        return out  # [u, v, p]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:20.608104Z","iopub.execute_input":"2025-11-06T15:03:20.608315Z","iopub.status.idle":"2025-11-06T15:03:20.617665Z","shell.execute_reply.started":"2025-11-06T15:03:20.608299Z","shell.execute_reply":"2025-11-06T15:03:20.616806Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_dir = \"/kaggle/input/cfd-flow-pass-a-cylinder-0-01\"\nt_start = 0\nt_end = 501\ndt = 0.01\n\nxyt_list = []\nuvp_list = []\n\nfor i in range(t_start, t_end):\n    csv_path = os.path.join(data_dir, f\"result_{i}.csv\")\n    df = pd.read_csv(csv_path)\n\n    # timestep\n    t_val = i * dt\n    t_column = np.full_like(df[\"Points:0\"].values, fill_value=t_val, dtype=np.float32)\n\n    # (x, y, t)\n    xyt = np.stack([\n        df[\"Points:0\"].values,\n        df[\"Points:1\"].values,\n        t_column\n    ], axis=1)  \n\n    # (u, v, p)\n    uvp = np.stack([\n        df[\"u:0\"].values,\n        df[\"u:1\"].values,\n        df[\"p\"].values\n    ], axis=1)  \n\n    xyt_list.append(xyt)\n    uvp_list.append(uvp)\n\nxyt_tensor = torch.tensor(np.concatenate(xyt_list, axis=0), dtype=torch.float32)\nuvp_tensor = torch.tensor(np.concatenate(uvp_list, axis=0), dtype=torch.float32)\n\nprint(xyt_tensor.shape)\nprint(uvp_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:20.618588Z","iopub.execute_input":"2025-11-06T15:03:20.618841Z","iopub.status.idle":"2025-11-06T15:03:51.631459Z","shell.execute_reply.started":"2025-11-06T15:03:20.618813Z","shell.execute_reply":"2025-11-06T15:03:51.630484Z"}},"outputs":[{"name":"stdout","text":"torch.Size([20806530, 3])\ntorch.Size([20806530, 3])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"time_col = xyt_tensor[:, 2]\n\n# Mask for t = 0\nmask_ic = (time_col == 0.0)\n\n# Apply mask\nxyt_tensor_ic = xyt_tensor[mask_ic]\nuvp_tensor_ic = uvp_tensor[mask_ic]\n\nprint(xyt_tensor_ic.shape)\nprint(uvp_tensor_ic.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.632234Z","iopub.execute_input":"2025-11-06T15:03:51.632690Z","iopub.status.idle":"2025-11-06T15:03:51.749656Z","shell.execute_reply.started":"2025-11-06T15:03:51.632663Z","shell.execute_reply":"2025-11-06T15:03:51.748802Z"}},"outputs":[{"name":"stdout","text":"torch.Size([41530, 3])\ntorch.Size([41530, 3])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def sample_points(inputs,\n                  outputs,\n                  N_per_timestep,\n                  seed=None,\n                  device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    obstacle_center=(0.5, 0.5)\n    obstacle_radius=0.05\n\n    # Refinement bounds and percents\n    refine_1 = ((0.35, 0.65), (0.35, 0.65))\n    percent_1 = 45\n\n    refine_2 = ((0.65, 2.5), (0.35, 0.65))\n    percent_2 = 35\n\n    percent_3 = 100 - (percent_1 + percent_2)\n\n    # Convert to numpy\n    pts_np = inputs.cpu().numpy()\n    uvp_np = outputs.cpu().numpy()\n\n    # Mask out obstacle region\n    dx = pts_np[:, 0] - obstacle_center[0]\n    dy = pts_np[:, 1] - obstacle_center[1]\n    mask = dx**2 + dy**2 >= obstacle_radius**2\n    pts_np = pts_np[mask]\n    uvp_np = uvp_np[mask]\n\n    # Timesteps\n    timesteps = np.unique(pts_np[:, 2])\n    timesteps = np.random.choice(timesteps, size=20, replace=False)\n\n    pts_final_list, uvp_final_list = [], []\n\n    for t in timesteps:\n        mask_t = np.abs(pts_np[:, 2] - t) < 1e-12\n        pts_t, uvp_t = pts_np[mask_t], uvp_np[mask_t]\n\n        # --- region 1\n        mask_r1 = (\n            (pts_t[:, 0] >= refine_1[0][0]) & (pts_t[:, 0] <= refine_1[0][1]) &\n            (pts_t[:, 1] >= refine_1[1][0]) & (pts_t[:, 1] <= refine_1[1][1])\n        )\n        pts_r1, uvp_r1 = pts_t[mask_r1], uvp_t[mask_r1]\n\n        # --- region 2\n        mask_r2 = (\n            (pts_t[:, 0] >= refine_2[0][0]) & (pts_t[:, 0] <= refine_2[0][1]) &\n            (pts_t[:, 1] >= refine_2[1][0]) & (pts_t[:, 1] <= refine_2[1][1])\n        )\n        pts_r2, uvp_r2 = pts_t[mask_r2], uvp_t[mask_r2]\n\n        # --- region 3 = rest\n        mask_r3 = ~(mask_r1 | mask_r2)\n        pts_r3, uvp_r3 = pts_t[mask_r3], uvp_t[mask_r3]\n\n        # How many per region\n        N1 = int(N_per_timestep * percent_1 / 100.0)\n        N2 = int(N_per_timestep * percent_2 / 100.0)\n        N3 = N_per_timestep - N1 - N2\n\n        def sample_region(pts_region, uvp_region, N):\n            if len(pts_region) == 0:  # if no points, fallback to random global\n                return np.empty((0, 3)), np.empty((0, 3))\n            if len(pts_region) < N:\n                idx = np.random.choice(len(pts_region), size=N, replace=True)\n            else:\n                idx = np.random.choice(len(pts_region), size=N, replace=False)\n            return pts_region[idx], uvp_region[idx]\n\n        # Sample each region\n        sp1, uv1 = sample_region(pts_r1, uvp_r1, N1)\n        sp2, uv2 = sample_region(pts_r2, uvp_r2, N2)\n        sp3, uv3 = sample_region(pts_r3, uvp_r3, N3)\n\n        pts_final_list.append(np.vstack([sp1, sp2, sp3]))\n        uvp_final_list.append(np.vstack([uv1, uv2, uv3]))\n\n    # Concatenate\n    pts_final = np.vstack(pts_final_list)\n    uvp_final = np.vstack(uvp_final_list)\n\n    # Shuffle\n    perm = np.random.permutation(len(pts_final))\n    pts_final = pts_final[perm]\n    uvp_final = uvp_final[perm]\n\n    # Torch tensors\n    points = torch.tensor(pts_final, dtype=torch.float32, device=device)\n    uvp = torch.tensor(uvp_final, dtype=torch.float32, device=device)\n    u = uvp[:, 0:1]\n    v = uvp[:, 1:2]\n    p = uvp[:, 2:3]\n\n    return points, u, v, p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.750533Z","iopub.execute_input":"2025-11-06T15:03:51.750872Z","iopub.status.idle":"2025-11-06T15:03:51.762889Z","shell.execute_reply.started":"2025-11-06T15:03:51.750852Z","shell.execute_reply":"2025-11-06T15:03:51.762111Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef sample_collocation_points(N_per_timestep,\n                                   seed=None,\n                                   device=\"cuda\"):\n\n\n    total_time=5.0\n    dt=0.01\n    n_timesteps_per_iter=20\n\n    \n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # --- Domain and geometry ---\n    domain_x, domain_y = (0, 2.5), (0, 1)\n    obstacle_center = (0.5, 0.5)\n    obstacle_radius2 = 0.05 ** 2\n\n    # --- Refinement regions ---\n    refine_1 = ((0.35, 0.65), (0.35, 0.65))\n    refine_2 = ((0.65, 2.5), (0.35, 0.65))\n    w1, w2 = 0.40, 0.35   # weights\n    w3 = 1.0 - (w1 + w2)\n\n    timesteps = np.arange(0, total_time + dt, dt)\n    # chosen_timesteps = np.random.choice(timesteps, size=n_timesteps_per_iter, replace=False)\n    chosen_timesteps = np.random.choice(timesteps, size=n_timesteps_per_iter, replace=False)\n\n    # --- Vectorized sampling ---\n    # Total number of samples overall\n    total_pts = N_per_timestep * n_timesteps_per_iter\n\n    # Assign which region each point belongs to\n    regions = np.random.choice(3, size=total_pts, p=[w1, w2, w3])\n\n    # Preallocate\n    x = np.empty(total_pts, dtype=np.float32)\n    y = np.empty(total_pts, dtype=np.float32)\n    t = np.repeat(chosen_timesteps, N_per_timestep).astype(np.float32)\n\n    # Region 1\n    mask1 = regions == 0\n    n1 = mask1.sum()\n    x[mask1] = np.random.uniform(*refine_1[0], n1)\n    y[mask1] = np.random.uniform(*refine_1[1], n1)\n\n    # Region 2\n    mask2 = regions == 1\n    n2 = mask2.sum()\n    x[mask2] = np.random.uniform(*refine_2[0], n2)\n    y[mask2] = np.random.uniform(*refine_2[1], n2)\n\n    # Region 3 (rest of domain)\n    mask3 = regions == 2\n    n3 = mask3.sum()\n    x[mask3] = np.random.uniform(*domain_x, n3)\n    y[mask3] = np.random.uniform(*domain_y, n3)\n\n    # --- Remove obstacle points (vectorized) ---\n    dx = x - obstacle_center[0]\n    dy = y - obstacle_center[1]\n    mask_keep = dx**2 + dy**2 >= obstacle_radius2\n\n    x, y, t = x[mask_keep], y[mask_keep], t[mask_keep]\n\n    # If too few points (after obstacle removal), resample fast (no loops)\n    n_needed = total_pts - len(x)\n    if n_needed > 0:\n        xr = np.random.uniform(*refine_2[0], n_needed)\n        yr = np.random.uniform(*refine_2[1], n_needed)\n        tr = np.random.choice(chosen_timesteps, size=n_needed)\n        dxr = xr - obstacle_center[0]\n        dyr = yr - obstacle_center[1]\n        keep = dxr**2 + dyr**2 >= obstacle_radius2\n        x = np.concatenate([x, xr[keep]])\n        y = np.concatenate([y, yr[keep]])\n        t = np.concatenate([t, tr[keep]])\n        x, y, t = x[:total_pts], y[:total_pts], t[:total_pts]\n\n    pts_np = np.stack([x, y, t], axis=1)\n    np.random.shuffle(pts_np)\n\n    return torch.tensor(pts_np, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.765223Z","iopub.execute_input":"2025-11-06T15:03:51.765427Z","iopub.status.idle":"2025-11-06T15:03:51.799088Z","shell.execute_reply.started":"2025-11-06T15:03:51.765411Z","shell.execute_reply":"2025-11-06T15:03:51.798453Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def sample_inlet(N_per_timestep,\n                 seed=None,\n                 device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    y_bounds = (0, 1)\n    x_in = 0.0\n\n    timesteps = np.arange(0, total_time + dt, dt)  # all time steps\n\n    all_points = []\n\n    for t in timesteps:\n        y = np.random.uniform(y_bounds[0], y_bounds[1], N_per_timestep)\n        x = np.full(N_per_timestep, x_in)\n        t_vals = np.full(N_per_timestep, t)\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.799632Z","iopub.execute_input":"2025-11-06T15:03:51.799830Z","iopub.status.idle":"2025-11-06T15:03:51.815950Z","shell.execute_reply.started":"2025-11-06T15:03:51.799813Z","shell.execute_reply":"2025-11-06T15:03:51.815377Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def sample_cylinder_surface(N_per_timestep,\n                            seed=None,\n                            device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    cyl_center = (0.5, 0.5)\n    cyl_radius = 0.05\n\n    timesteps = np.arange(0, total_time + dt, dt)\n    all_points = []\n    cx, cy = cyl_center\n\n    for t in timesteps:\n        # Sample angles uniformly around the circle\n        theta = np.random.uniform(0, 2*np.pi, N_per_timestep)\n        x = cx + cyl_radius * np.cos(theta)\n        y = cy + cyl_radius * np.sin(theta)\n        t_vals = np.full(N_per_timestep, t)\n\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.816598Z","iopub.execute_input":"2025-11-06T15:03:51.816839Z","iopub.status.idle":"2025-11-06T15:03:51.831381Z","shell.execute_reply.started":"2025-11-06T15:03:51.816819Z","shell.execute_reply":"2025-11-06T15:03:51.830781Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def sample_top_bottom(N_per_timestep,\n                      seed=None,\n                      device=\"cpu\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    total_time = 5.0\n    dt = 0.01\n    x_bounds = (0, 2.5)\n    y_top = 1.0\n    y_bot = 0.0\n\n    timesteps = np.arange(0, total_time + dt, dt)\n\n    # Split evenly between top and bottom\n    N_top = N_per_timestep // 2\n    N_bot = N_per_timestep - N_top  # handle odd numbers\n\n    all_points = []\n\n    for t in timesteps:\n        # Top wall\n        x_top = np.random.uniform(x_bounds[0], x_bounds[1], N_top)\n        y_top_arr = np.full(N_top, y_top)\n        t_top = np.full(N_top, t)\n        pts_top = np.stack([x_top, y_top_arr, t_top], axis=1)\n\n        # Bottom wall\n        x_bot = np.random.uniform(x_bounds[0], x_bounds[1], N_bot)\n        y_bot_arr = np.full(N_bot, y_bot)\n        t_bot = np.full(N_bot, t)\n        pts_bot = np.stack([x_bot, y_bot_arr, t_bot], axis=1)\n\n        all_points.append(pts_top)\n        all_points.append(pts_bot)\n\n    all_points = np.vstack(all_points)\n\n    # Shuffle\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.832142Z","iopub.execute_input":"2025-11-06T15:03:51.832383Z","iopub.status.idle":"2025-11-06T15:03:51.844567Z","shell.execute_reply.started":"2025-11-06T15:03:51.832367Z","shell.execute_reply":"2025-11-06T15:03:51.843826Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def sample_initial(N,\n                   xyt_tensor_ic, \n                   uvp_tensor_ic,\n                   device=\"cuda\"):\n\n\n    # Cylinder info\n    cyl_center = (0.5, 0.5)\n    cyl_radius = 0.05\n\n    # Hardcoded bounds\n    bound1 = ((0.35, 0.65), (0.35, 0.65))  # region 1 (around cylinder)\n    bound2 = ((0.65, 2.5), (0.35, 0.65))   # region 2 (wake)\n    \n    # Hardcoded percentages\n    perc_r1, perc_r2, perc_r3 = 0.5, 0.3, 0.2\n\n    N_r1 = int(N * perc_r1)\n    N_r2 = int(N * perc_r2)\n    N_r3 = N - N_r1 - N_r2  # ensure exact total\n\n    # Unpack coordinates\n    x = xyt_tensor_ic[:, 0]\n    y = xyt_tensor_ic[:, 1]\n\n    # Mask out cylinder\n    cx, cy = cyl_center\n    dx, dy = x - cx, y - cy\n    mask_cyl = (dx**2 + dy**2) >= cyl_radius**2\n    xyt_valid = xyt_tensor_ic[mask_cyl]\n    uvp_valid = uvp_tensor_ic[mask_cyl]\n\n    # --- Region 1 (refinement box around cylinder) ---\n    mask_r1 = ((xyt_valid[:, 0] >= bound1[0][0]) & (xyt_valid[:, 0] <= bound1[0][1]) &\n               (xyt_valid[:, 1] >= bound1[1][0]) & (xyt_valid[:, 1] <= bound1[1][1]))\n\n    # --- Region 2 (wake region) ---\n    mask_r2 = ((xyt_valid[:, 0] >= bound2[0][0]) & (xyt_valid[:, 0] <= bound2[0][1]) &\n               (xyt_valid[:, 1] >= bound2[1][0]) & (xyt_valid[:, 1] <= bound2[1][1]))\n\n    # --- Region 3 = rest ---\n    mask_r3 = ~(mask_r1 | mask_r2)\n\n    # Helper: sample points from region\n    def sample_region(mask, N):\n        xyt_reg = xyt_valid[mask]\n        uvp_reg = uvp_valid[mask]\n        if len(xyt_reg) == 0:\n            return None, None\n        replace = xyt_reg.shape[0] < N\n        idx = torch.randint(0, xyt_reg.shape[0], (N,), device=\"cpu\", dtype=torch.long) if replace else \\\n                torch.randperm(xyt_reg.shape[0])[:N]\n\n        return xyt_reg[idx].to(device), uvp_reg[idx].to(device)\n\n    # Sample each region\n    xyt_r1, uvp_r1 = sample_region(mask_r1, N_r1)\n    xyt_r2, uvp_r2 = sample_region(mask_r2, N_r2)\n    xyt_r3, uvp_r3 = sample_region(mask_r3, N_r3)\n\n    # Concatenate\n    sampled_xyt = torch.cat([t for t in [xyt_r1, xyt_r2, xyt_r3] if t is not None], dim=0)\n    sampled_uvp = torch.cat([t for t in [uvp_r1, uvp_r2, uvp_r3] if t is not None], dim=0)\n\n    return sampled_xyt, sampled_uvp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.845286Z","iopub.execute_input":"2025-11-06T15:03:51.845547Z","iopub.status.idle":"2025-11-06T15:03:51.861745Z","shell.execute_reply.started":"2025-11-06T15:03:51.845523Z","shell.execute_reply":"2025-11-06T15:03:51.861013Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def compute_residuals(model, X):\n\n    nu = 5e-4            \n    rho = 1\n\n    X = X.clone().detach().requires_grad_(True)\n    out = model(X)\n    u = out[:,0:1]\n    v = out[:,1:2]\n    p = out[:,2:3]\n\n    grads_u = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    u_x = grads_u[:,0:1]\n    u_y = grads_u[:,1:2]\n    u_t = grads_u[:,2:3]\n\n    grads_v = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n    v_x = grads_v[:,0:1]\n    v_y = grads_v[:,1:2]\n    v_t = grads_v[:,2:3]\n\n    grads_p = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n    p_x = grads_p[:,0:1]\n    p_y = grads_p[:,1:2]\n\n    # second derivatives\n    grads_ux = torch.autograd.grad(grads_u, X, torch.ones_like(grads_u), create_graph=True)[0]\n    grads_vx = torch.autograd.grad(grads_v, X, torch.ones_like(grads_v), create_graph=True)[0]\n    u_xx, u_yy = grads_ux[:,0:1], grads_ux[:,1:2]\n    v_xx, v_yy = grads_vx[:,0:1], grads_vx[:,1:2]\n\n    # continuity\n    cont = u_x + v_y\n\n    # momentum equations\n    mom_u = u_t + (u * u_x + v * u_y) + (1/rho) * p_x - nu * (u_xx + u_yy)\n    mom_v = v_t + (u * v_x + v * v_y) + (1/rho) * p_y - nu * (v_xx + v_yy)\n\n\n    return cont, mom_u, mom_v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.862545Z","iopub.execute_input":"2025-11-06T15:03:51.862778Z","iopub.status.idle":"2025-11-06T15:03:51.878792Z","shell.execute_reply.started":"2025-11-06T15:03:51.862756Z","shell.execute_reply":"2025-11-06T15:03:51.878237Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def data_loss(model, pts, u, v, p, device):\n\n    # Model prediction\n    pred = model(pts)\n    u_pred, v_pred, p_pred = pred[:,0:1], pred[:,1:2], pred[:,2:3]\n\n    mse = nn.MSELoss()\n    return mse(u_pred, u) + mse(v_pred, v) + mse(p_pred, p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.879385Z","iopub.execute_input":"2025-11-06T15:03:51.879592Z","iopub.status.idle":"2025-11-06T15:03:51.896100Z","shell.execute_reply.started":"2025-11-06T15:03:51.879577Z","shell.execute_reply":"2025-11-06T15:03:51.895469Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"os.makedirs('models', exist_ok=True)\nNf, Nf_data, Nic, Nd, Nn, Ni = 40000, 10000, 50000, 60, 700, 150","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.896755Z","iopub.execute_input":"2025-11-06T15:03:51.896989Z","iopub.status.idle":"2025-11-06T15:03:51.910534Z","shell.execute_reply.started":"2025-11-06T15:03:51.896952Z","shell.execute_reply":"2025-11-06T15:03:51.909994Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def compute_residuals_in_batches(model, X, batch_size=10000):\n    \"\"\"\n    Compute PDE residuals in mini-batches to avoid CUDA OOM.\n    \"\"\"\n    cont_list, mu_list, mv_list = [], [], []\n    n = X.size(0)\n\n    for i in range(0, n, batch_size):\n        X_batch = X[i:i+batch_size].clone().detach().requires_grad_(True)\n        cont, mu, mv = compute_residuals(model, X_batch)\n\n        cont_list.append(cont.detach())\n        mu_list.append(mu.detach())\n        mv_list.append(mv.detach())\n\n        # free up memory\n        del cont, mu, mv, X_batch\n        torch.cuda.empty_cache()\n\n    # concatenate results back\n    return (torch.cat(cont_list, dim=0),\n            torch.cat(mu_list, dim=0),\n            torch.cat(mv_list, dim=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.911114Z","iopub.execute_input":"2025-11-06T15:03:51.911308Z","iopub.status.idle":"2025-11-06T15:03:51.924862Z","shell.execute_reply.started":"2025-11-06T15:03:51.911295Z","shell.execute_reply":"2025-11-06T15:03:51.924289Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = ResNetPINN().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:51.925581Z","iopub.execute_input":"2025-11-06T15:03:51.925809Z","iopub.status.idle":"2025-11-06T15:03:52.110630Z","shell.execute_reply.started":"2025-11-06T15:03:51.925790Z","shell.execute_reply":"2025-11-06T15:03:52.110016Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\nfrom torch.cuda.amp import autocast, GradScaler\n\n\nmse_loss = nn.MSELoss()\nmae_loss = nn.L1Loss()\n\ndef train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=40000, print_every=2000, save_every=2000,\n               λ_data=1.0, alpha_pde=1.0, seed=None, device=\"cuda\",\n               checkpoint_dir=\"models\"):\n\n    print(\"Start training loop\")\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scaler = GradScaler()\n\n    start_iter = 0\n\n    # --- Try to resume from the latest checkpoint ---\n    latest_ckpt = None\n    if os.path.exists(checkpoint_dir):\n        ckpts = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"pinn_checkpoint_\")]\n        if ckpts:\n            latest_ckpt = max(ckpts, key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0]))\n\n    if latest_ckpt:\n        path = os.path.join(checkpoint_dir, latest_ckpt)\n        print(f\"Loading checkpoint from {path}...\")\n        checkpoint = torch.load(path, map_location=device)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        start_iter = checkpoint[\"iter\"] + 1\n        print(f\"Resuming training from iteration {start_iter}\")\n\n\n    for it in range(start_iter, num_iters):\n        optimizer.zero_grad()\n\n        with autocast():\n\n            # Physics residual loss\n            X_f = sample_collocation_points(N_per_timestep=Nf,\n                                              seed=None,\n                                              device=\"cuda\")\n            \n            X_f_data, u, v, p = sample_points(inputs=xyt_tensor,\n                                              outputs=uvp_tensor,\n                                              N_per_timestep=Nf_data,\n                                              seed=None,\n                                              device=\"cuda\")\n        \n    \n    \n            cont, mu, mv = compute_residuals_in_batches(model, X_f, batch_size=10000)\n            \n            loss_f = (mae_loss(cont, torch.zeros_like(cont)) + mae_loss(mu, torch.zeros_like(mu)) + mae_loss(mv, torch.zeros_like(mv)))\n    \n    \n            # Data loss\n            loss_data = data_loss(model, X_f_data, u, v, p, device=\"cuda\")\n    \n    \n            # Initial condition\n            X_ic, Y_ic = sample_initial(N=Nic,xyt_tensor_ic=xyt_tensor_ic, uvp_tensor_ic=uvp_tensor_ic, device=\"cuda\")\n    \n            \n            out_ic = model(X_ic)\n            loss_ic = (mse_loss(out_ic[:, 0:1], Y_ic[:, 0:1]) + mse_loss(out_ic[:, 1:2], Y_ic[:, 1:2]) + mse_loss(out_ic[:, 2:3], Y_ic[:, 2:3]))\n    \n    \n            # Inlet condition\n            X_in = sample_inlet(N_per_timestep=Ni, seed=None, device=\"cuda\")\n    \n            \n            out_in = model(X_in)\n            loss_in = (mse_loss(out_in[:, 0:1], torch.ones_like(out_in[:, 0:1])) + mse_loss(out_in[:, 1:2], torch.zeros_like(out_in[:, 1:2])))\n    \n    \n            # Cylinder surface\n            X_cyl = sample_cylinder_surface(N_per_timestep=Nd, seed=None, device=\"cuda\")\n    \n            \n            out_cyl = model(X_cyl)\n            loss_cyl = (mse_loss(out_cyl[:, 0:1], torch.zeros_like(out_cyl[:, 0:1])) + mse_loss(out_cyl[:, 1:2], torch.zeros_like(out_cyl[:, 1:2])))\n        \n    \n            # Top/bottom boundaries\n            X_tb = sample_top_bottom(N_per_timestep=Nn, seed=None, device=\"cuda\")\n    \n            out_tb = model(X_tb)\n            loss_tb = (mse_loss(out_tb[:, 1:2], torch.zeros_like(out_tb[:, 1:2])))\n    \n    \n            # Total loss\n            loss = alpha_pde * loss_f + λ_data * loss_data + loss_ic + loss_in + loss_cyl + loss_tb\n\n\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        torch.cuda.empty_cache()\n\n\n        if it % print_every == 0:\n            print(f\"Adam Iter {it}/{num_iters} - Total loss {loss.item():.6f}\\n\"\n                  f\"Data {loss_data.item():.6f} | PDE {loss_f.item():.4e}\\n\"\n                  f\"IC {loss_ic.item():.4e}, Inlet {loss_in.item():.4e}\\n\"\n                  f\"Cylinder {loss_cyl.item():.4e}, TopBottom {loss_tb.item():.4e}\")\n\n         # --- Save checkpoint every save_every iterations ---\n        if (it + 1) % save_every == 0:\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            ckpt_path = os.path.join(checkpoint_dir, f\"resnet_pinn_checkpoint_{it+1}.pth\")\n            torch.save({\n                \"iter\": it,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scaler_state\": scaler.state_dict()\n            }, ckpt_path)\n            print(f\"Checkpoint saved → {ckpt_path}\")\n\n         \n            \n\n    print(f\"Adam Iter {num_iters-1} - Total loss {loss.item():.6f}\\n\"\n              f\"Data {loss_data.item():.6f} | PDE {loss_f.item():.4e}\\n\"\n              f\"IC {loss_ic.item():.4e}, Inlet {loss_in.item():.4e}\\n\"\n              f\"Cylinder {loss_cyl.item():.4e}, TopBottom {loss_tb.item():.4e}\")\n\n\n    # Final model save\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    final_path = os.path.join(checkpoint_dir, \"resnet_pinn_ns_adam_final.pth\")\n    torch.save(model.state_dict(), final_path)\n    print(f\"Final model saved at {final_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:52.111317Z","iopub.execute_input":"2025-11-06T15:03:52.111539Z","iopub.status.idle":"2025-11-06T15:03:52.126752Z","shell.execute_reply.started":"2025-11-06T15:03:52.111523Z","shell.execute_reply":"2025-11-06T15:03:52.126203Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=3000, print_every=10, save_every=100,\n               λ_data=1.0, alpha_pde=1.0, seed=None, device=\"cuda\",\n               checkpoint_dir=\"models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:03:52.127511Z","iopub.execute_input":"2025-11-06T15:03:52.128225Z","iopub.status.idle":"2025-11-06T15:57:42.757415Z","shell.execute_reply.started":"2025-11-06T15:03:52.128208Z","shell.execute_reply":"2025-11-06T15:57:42.756484Z"}},"outputs":[{"name":"stdout","text":"Start training loop\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_46/3597703033.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_46/3597703033.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Adam Iter 0/3000 - Total loss 136.875443\nData 44.054211 | PDE 1.2036e+01\nIC 3.2296e+01, Inlet 1.7896e+01\nCylinder 2.2676e+01, TopBottom 7.9169e+00\nAdam Iter 10/3000 - Total loss 642.663574\nData 184.005219 | PDE 4.6360e+01\nIC 9.1406e+01, Inlet 1.7070e+02\nCylinder 1.2545e+02, TopBottom 2.4741e+01\nAdam Iter 20/3000 - Total loss 8.557334\nData 2.311998 | PDE 2.6549e+00\nIC 1.5267e+00, Inlet 1.4089e+00\nCylinder 3.6732e-01, TopBottom 2.8745e-01\nAdam Iter 30/3000 - Total loss 5.392131\nData 1.523679 | PDE 1.6828e+00\nIC 3.9847e-01, Inlet 4.4518e-01\nCylinder 7.5360e-01, TopBottom 5.8845e-01\nAdam Iter 40/3000 - Total loss 3.175578\nData 0.739210 | PDE 1.6358e+00\nIC 3.6271e-01, Inlet 1.9639e-01\nCylinder 1.1812e-01, TopBottom 1.2336e-01\nAdam Iter 50/3000 - Total loss 2.281927\nData 0.511267 | PDE 1.2822e+00\nIC 3.2813e-01, Inlet 6.4624e-02\nCylinder 7.3811e-02, TopBottom 2.1885e-02\nAdam Iter 60/3000 - Total loss 2.363166\nData 0.402008 | PDE 1.6140e+00\nIC 1.4531e-01, Inlet 2.2630e-02\nCylinder 1.6382e-01, TopBottom 1.5368e-02\nAdam Iter 70/3000 - Total loss 2.265800\nData 0.377929 | PDE 1.5916e+00\nIC 1.1347e-01, Inlet 9.4439e-03\nCylinder 1.6410e-01, TopBottom 9.2686e-03\nAdam Iter 80/3000 - Total loss 1.866816\nData 0.429925 | PDE 1.2117e+00\nIC 1.0393e-01, Inlet 1.6522e-02\nCylinder 9.7208e-02, TopBottom 7.5724e-03\nAdam Iter 90/3000 - Total loss 1.871953\nData 0.410747 | PDE 1.2394e+00\nIC 1.0328e-01, Inlet 6.8910e-03\nCylinder 1.0581e-01, TopBottom 5.8602e-03\nCheckpoint saved → models/resnet_pinn_checkpoint_100.pth\nAdam Iter 100/3000 - Total loss 2.190840\nData 0.366310 | PDE 1.5982e+00\nIC 9.9259e-02, Inlet 6.8867e-03\nCylinder 1.1427e-01, TopBottom 5.8862e-03\nAdam Iter 110/3000 - Total loss 1.902127\nData 0.386094 | PDE 1.3139e+00\nIC 9.9641e-02, Inlet 5.5783e-03\nCylinder 9.2900e-02, TopBottom 3.9816e-03\nAdam Iter 120/3000 - Total loss 1.758070\nData 0.361985 | PDE 1.1738e+00\nIC 9.7143e-02, Inlet 3.7063e-03\nCylinder 1.1751e-01, TopBottom 3.9370e-03\nAdam Iter 130/3000 - Total loss 1.775402\nData 0.371353 | PDE 1.2079e+00\nIC 9.7154e-02, Inlet 8.8508e-03\nCylinder 8.6260e-02, TopBottom 3.9266e-03\nAdam Iter 140/3000 - Total loss 1.819786\nData 0.344136 | PDE 1.2436e+00\nIC 9.6564e-02, Inlet 5.9534e-03\nCylinder 1.2589e-01, TopBottom 3.6466e-03\nAdam Iter 150/3000 - Total loss 1.944126\nData 0.367230 | PDE 1.3559e+00\nIC 9.8181e-02, Inlet 7.0239e-03\nCylinder 1.1271e-01, TopBottom 3.1067e-03\nAdam Iter 160/3000 - Total loss 1.955214\nData 0.348074 | PDE 1.3785e+00\nIC 9.6159e-02, Inlet 3.9614e-03\nCylinder 1.2558e-01, TopBottom 2.9028e-03\nAdam Iter 170/3000 - Total loss 1.941600\nData 0.301310 | PDE 1.3215e+00\nIC 1.0551e-01, Inlet 2.2972e-02\nCylinder 1.8419e-01, TopBottom 6.1063e-03\nAdam Iter 180/3000 - Total loss 1.998545\nData 0.487218 | PDE 1.2782e+00\nIC 1.6043e-01, Inlet 2.0629e-02\nCylinder 4.0750e-02, TopBottom 1.1349e-02\nAdam Iter 190/3000 - Total loss 1.882398\nData 0.353109 | PDE 1.2930e+00\nIC 1.0293e-01, Inlet 6.2692e-03\nCylinder 1.2308e-01, TopBottom 4.0451e-03\nCheckpoint saved → models/resnet_pinn_checkpoint_200.pth\nAdam Iter 200/3000 - Total loss 1.958382\nData 0.410468 | PDE 1.3441e+00\nIC 1.2761e-01, Inlet 9.2408e-03\nCylinder 6.4058e-02, TopBottom 2.9392e-03\nAdam Iter 210/3000 - Total loss 1.911653\nData 0.326839 | PDE 1.3367e+00\nIC 9.5024e-02, Inlet 7.2399e-03\nCylinder 1.4016e-01, TopBottom 5.7329e-03\nAdam Iter 220/3000 - Total loss 2.031034\nData 0.397995 | PDE 1.4504e+00\nIC 9.3248e-02, Inlet 1.2993e-02\nCylinder 7.4059e-02, TopBottom 2.3418e-03\nAdam Iter 230/3000 - Total loss 2.123330\nData 0.316683 | PDE 1.5659e+00\nIC 9.9377e-02, Inlet 8.5986e-03\nCylinder 1.2925e-01, TopBottom 3.5547e-03\nAdam Iter 240/3000 - Total loss 1.951517\nData 0.387550 | PDE 1.3605e+00\nIC 9.7558e-02, Inlet 1.7946e-02\nCylinder 8.5918e-02, TopBottom 2.0225e-03\nAdam Iter 250/3000 - Total loss 1.967447\nData 0.396053 | PDE 1.3910e+00\nIC 9.6614e-02, Inlet 6.6903e-03\nCylinder 7.2726e-02, TopBottom 4.3679e-03\nAdam Iter 260/3000 - Total loss 1.981304\nData 0.381172 | PDE 1.4278e+00\nIC 1.0173e-01, Inlet 4.4931e-03\nCylinder 6.4094e-02, TopBottom 2.0547e-03\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_46/2604798588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                \u001b[0mλ_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_pde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                checkpoint_dir=\"models\")\n","\u001b[0;32m/tmp/ipykernel_46/3597703033.py\u001b[0m in \u001b[0;36mtrain_adam\u001b[0;34m(model, Nf, Nf_data, Nic, Nd, Nn, Ni, num_iters, print_every, save_every, λ_data, alpha_pde, seed, device, checkpoint_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mcont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_residuals_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_46/443383566.py\u001b[0m in \u001b[0;36mcompute_residuals_in_batches\u001b[0;34m(model, X, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# free up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mcont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# concatenate results back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}