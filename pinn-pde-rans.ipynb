{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13449197,"sourceType":"datasetVersion","datasetId":8536862}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import griddata\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport math\nimport random\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:05:57.573637Z","iopub.execute_input":"2025-11-24T09:05:57.574190Z","iopub.status.idle":"2025-11-24T09:05:57.578405Z","shell.execute_reply.started":"2025-11-24T09:05:57.574165Z","shell.execute_reply":"2025-11-24T09:05:57.577630Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"nu = 5e-4            \nrho = 1\nU_inlet = 1.0\n\ncyl_center = (0.5, 0.5)\ncyl_radius = 0.05\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:05:57.580035Z","iopub.execute_input":"2025-11-24T09:05:57.580315Z","iopub.status.idle":"2025-11-24T09:05:57.600261Z","shell.execute_reply.started":"2025-11-24T09:05:57.580290Z","shell.execute_reply":"2025-11-24T09:05:57.599494Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\ndef xavier_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\nclass PINN(nn.Module):\n    def __init__(self, layers=[3] + [64]*10 + [4]):\n        \"\"\"\n        Tanh-based PINN for URANS + RNG k-ε model.\n        Input: [x, y, t]\n        Output: [ψ, p, k, ε]\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for i in range(len(layers) - 2):\n            self.layers.append(nn.Linear(layers[i], layers[i + 1]))\n        self.output_layer = nn.Linear(layers[-2], layers[-1])\n        \n        # Activation\n        self.activation = nn.Tanh()\n\n        # Initialize weights \n        self.apply(xavier_init)\n\n    def forward(self, x):\n        \"\"\"Forward pass: returns ψ, p, k, ε\"\"\"\n        h = x\n        for layer in self.layers:\n            h = self.activation(layer(h))\n        out = self.output_layer(h)\n        ψ = out[:, 0:1]\n        p = out[:, 1:2] \n        self.softplus = nn.Softplus() \n        k = self.softplus(out[:, 2:3]) + 1e-6 \n        eps = self.softplus(out[:, 3:4]) + 1e-6 \n        return ψ, p, k, eps\n\n    def velocity_pressure(self, x):\n        \"\"\"Compute velocity (u,v) from streamfunction ψ.\"\"\"\n        x.requires_grad_(True)\n        ψ, p, k, eps = self.forward(x)\n        grads = torch.autograd.grad(\n            ψ, x, grad_outputs=torch.ones_like(ψ),\n            create_graph=True,\n            retain_graph=True\n        )[0]\n        ψ_x = grads[:, 0:1]\n        ψ_y = grads[:, 1:2]\n        u = ψ_y\n        v = -ψ_x\n        return u, v, p\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:05:57.601167Z","iopub.execute_input":"2025-11-24T09:05:57.601359Z","iopub.status.idle":"2025-11-24T09:05:57.621555Z","shell.execute_reply.started":"2025-11-24T09:05:57.601346Z","shell.execute_reply":"2025-11-24T09:05:57.620843Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"data_dir = \"/kaggle/input/cfd-flow-pass-a-cylinder-0-01\"\nt_start = 0\nt_end = 501\ndt = 0.01\n\nxyt_list = []\nuvp_list = []\n\nfor i in range(t_start, t_end):\n    csv_path = os.path.join(data_dir, f\"result_{i}.csv\")\n    df = pd.read_csv(csv_path)\n\n    # timestep\n    t_val = i * dt\n    t_column = np.full_like(df[\"Points:0\"].values, fill_value=t_val, dtype=np.float32)\n\n    # (x, y, t)\n    xyt = np.stack([\n        df[\"Points:0\"].values,\n        df[\"Points:1\"].values,\n        t_column\n    ], axis=1)  \n\n    # (u, v, p)\n    uvp = np.stack([\n        df[\"u:0\"].values,\n        df[\"u:1\"].values,\n        df[\"p\"].values\n    ], axis=1)  \n\n    xyt_list.append(xyt)\n    uvp_list.append(uvp)\n\nxyt_tensor = torch.tensor(np.concatenate(xyt_list, axis=0), dtype=torch.float32)\nuvp_tensor = torch.tensor(np.concatenate(uvp_list, axis=0), dtype=torch.float32)\n\nprint(xyt_tensor.shape)\nprint(uvp_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:05:57.622267Z","iopub.execute_input":"2025-11-24T09:05:57.622499Z","iopub.status.idle":"2025-11-24T09:06:16.036015Z","shell.execute_reply.started":"2025-11-24T09:05:57.622480Z","shell.execute_reply":"2025-11-24T09:06:16.035396Z"}},"outputs":[{"name":"stdout","text":"torch.Size([20806530, 3])\ntorch.Size([20806530, 3])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\n\n# Assuming xyt_tensor is your (N, 3) tensor [x, y, t]\nxyt_scaled = xyt_tensor.clone()\n\n# Compute min and max for each column\nmins = xyt_tensor.min(dim=0)[0]\nmaxs = xyt_tensor.max(dim=0)[0]\n\n# Scale each dimension to [-1, 1]\nxyt_scaled = 2 * (xyt_tensor - mins) / (maxs - mins) - 1\n\nprint(\"Scaled shape:\", xyt_scaled.shape)\nprint(\"Min values:\", xyt_scaled.min(dim=0)[0])\nprint(\"Max values:\", xyt_scaled.max(dim=0)[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:16.037766Z","iopub.execute_input":"2025-11-24T09:06:16.037981Z","iopub.status.idle":"2025-11-24T09:06:16.934845Z","shell.execute_reply.started":"2025-11-24T09:06:16.037964Z","shell.execute_reply":"2025-11-24T09:06:16.934057Z"}},"outputs":[{"name":"stdout","text":"Scaled shape: torch.Size([20806530, 3])\nMin values: tensor([-1., -1., -1.])\nMax values: tensor([1., 1., 1.])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"scaling_params = {\n    \"mins\": mins,\n    \"maxs\": maxs\n}\n\n# Example of inverse transform (from scaled → original)\ndef inverse_scale(x_scaled, mins, maxs):\n    return 0.5 * (x_scaled + 1) * (maxs - mins) + mins","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:16.935647Z","iopub.execute_input":"2025-11-24T09:06:16.935859Z","iopub.status.idle":"2025-11-24T09:06:16.939954Z","shell.execute_reply.started":"2025-11-24T09:06:16.935845Z","shell.execute_reply":"2025-11-24T09:06:16.939231Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"t_min, t_max = mins[2], maxs[2]\nt0_scaled = 2 * (0.0 - t_min) / (t_max - t_min) - 1\nprint(\"Scaled t=0 corresponds to:\", t0_scaled.item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:16.940672Z","iopub.execute_input":"2025-11-24T09:06:16.940853Z","iopub.status.idle":"2025-11-24T09:06:16.957955Z","shell.execute_reply.started":"2025-11-24T09:06:16.940839Z","shell.execute_reply":"2025-11-24T09:06:16.957295Z"}},"outputs":[{"name":"stdout","text":"Scaled t=0 corresponds to: -1.0\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"time_col_scaled = xyt_scaled[:, 2]\n\n# Use tolerance instead of exact equality\nmask_ic = torch.isclose(time_col_scaled, torch.tensor(t0_scaled, dtype=time_col_scaled.dtype), atol=1e-6)\n\nxyt_tensor_ic = xyt_scaled[mask_ic]\nuvp_tensor_ic = uvp_tensor[mask_ic]\n\nprint(\"IC xyt:\", xyt_tensor_ic.shape)\nprint(\"IC uvp:\", uvp_tensor_ic.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:16.958650Z","iopub.execute_input":"2025-11-24T09:06:16.958942Z","iopub.status.idle":"2025-11-24T09:06:17.282344Z","shell.execute_reply.started":"2025-11-24T09:06:16.958917Z","shell.execute_reply":"2025-11-24T09:06:17.281495Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_39/2071658522.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mask_ic = torch.isclose(time_col_scaled, torch.tensor(t0_scaled, dtype=time_col_scaled.dtype), atol=1e-6)\n","output_type":"stream"},{"name":"stdout","text":"IC xyt: torch.Size([41530, 3])\nIC uvp: torch.Size([41530, 3])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def sample_points(inputs,\n                  outputs,\n                  N_per_timestep,\n                  seed=None,\n                  device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # ⚠️ Adjusted for scaled coordinates [-1, 1]\n    # Original domain: x ∈ [0, 2.5], y ∈ [0, 1]\n    # After scaling: x_scaled = 2*(x/Lx) - 1, y_scaled = 2*(y/Ly) - 1\n    # So:\n    #   x=0   → -1\n    #   x=2.5 →  1\n    #   y=0   → -1\n    #   y=1   →  1\n\n    # Convert all physical coordinates into scaled space manually if needed\n    # but assuming inputs are already scaled to [-1, 1]:\n\n    # Obstacle (original center (0.5, 0.5), radius 0.05)\n    obstacle_center = (\n        2 * (0.5 / 2.5) - 1,  # scaled x center\n        2 * (0.5 / 1.0) - 1   # scaled y center\n    )\n    obstacle_radius = 2 * (0.05 / 2.5)  # scaled by x-domain\n\n    # Regions (convert from physical to scaled)\n    def scale_x(x): return 2 * (x / 2.5) - 1\n    def scale_y(y): return 2 * (y / 1.0) - 1\n\n    refine_1 = ((scale_x(0.35), scale_x(0.65)), (scale_y(0.35), scale_y(0.65)))\n    refine_2 = ((scale_x(0.65), scale_x(2.5)), (scale_y(0.35), scale_y(0.65)))\n\n    percent_1 = 40\n    percent_2 = 35\n    percent_3 = 100 - (percent_1 + percent_2)\n\n    # Convert to numpy\n    pts_np = inputs.cpu().numpy()\n    uvp_np = outputs.cpu().numpy()\n\n    # Mask out obstacle region\n    dx = pts_np[:, 0] - obstacle_center[0]\n    dy = pts_np[:, 1] - obstacle_center[1]\n    mask = dx**2 + dy**2 >= obstacle_radius**2\n    pts_np = pts_np[mask]\n    uvp_np = uvp_np[mask]\n\n    # Timesteps\n    timesteps = np.unique(pts_np[:, 2])\n    timesteps = np.random.choice(timesteps, size=20, replace=False)\n\n    pts_final_list, uvp_final_list = [], []\n\n    for t in timesteps:\n        mask_t = np.abs(pts_np[:, 2] - t) < 1e-12\n        pts_t, uvp_t = pts_np[mask_t], uvp_np[mask_t]\n\n        # --- region 1\n        mask_r1 = (\n            (pts_t[:, 0] >= refine_1[0][0]) & (pts_t[:, 0] <= refine_1[0][1]) &\n            (pts_t[:, 1] >= refine_1[1][0]) & (pts_t[:, 1] <= refine_1[1][1])\n        )\n        pts_r1, uvp_r1 = pts_t[mask_r1], uvp_t[mask_r1]\n\n        # --- region 2\n        mask_r2 = (\n            (pts_t[:, 0] >= refine_2[0][0]) & (pts_t[:, 0] <= refine_2[0][1]) &\n            (pts_t[:, 1] >= refine_2[1][0]) & (pts_t[:, 1] <= refine_2[1][1])\n        )\n        pts_r2, uvp_r2 = pts_t[mask_r2], uvp_t[mask_r2]\n\n        # --- region 3 = rest\n        mask_r3 = ~(mask_r1 | mask_r2)\n        pts_r3, uvp_r3 = pts_t[mask_r3], uvp_t[mask_r3]\n\n        # How many per region\n        N1 = int(N_per_timestep * percent_1 / 100.0)\n        N2 = int(N_per_timestep * percent_2 / 100.0)\n        N3 = N_per_timestep - N1 - N2\n\n        def sample_region(pts_region, uvp_region, N):\n            if len(pts_region) == 0:\n                return np.empty((0, 3)), np.empty((0, 3))\n            if len(pts_region) < N:\n                idx = np.random.choice(len(pts_region), size=N, replace=True)\n            else:\n                idx = np.random.choice(len(pts_region), size=N, replace=False)\n            return pts_region[idx], uvp_region[idx]\n\n        # Sample each region\n        sp1, uv1 = sample_region(pts_r1, uvp_r1, N1)\n        sp2, uv2 = sample_region(pts_r2, uvp_r2, N2)\n        sp3, uv3 = sample_region(pts_r3, uvp_r3, N3)\n\n        pts_final_list.append(np.vstack([sp1, sp2, sp3]))\n        uvp_final_list.append(np.vstack([uv1, uv2, uv3]))\n\n    # Concatenate\n    pts_final = np.vstack(pts_final_list)\n    uvp_final = np.vstack(uvp_final_list)\n\n    # Shuffle\n    perm = np.random.permutation(len(pts_final))\n    pts_final = pts_final[perm]\n    uvp_final = uvp_final[perm]\n\n    # Torch tensors\n    points = torch.tensor(pts_final, dtype=torch.float32, device=device)\n    uvp = torch.tensor(uvp_final, dtype=torch.float32, device=device)\n    u = uvp[:, 0:1]\n    v = uvp[:, 1:2]\n    p = uvp[:, 2:3]\n\n    return points, u, v, p\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.283229Z","iopub.execute_input":"2025-11-24T09:06:17.283482Z","iopub.status.idle":"2025-11-24T09:06:17.296897Z","shell.execute_reply.started":"2025-11-24T09:06:17.283465Z","shell.execute_reply":"2025-11-24T09:06:17.296203Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef sample_collocation_points(N_per_timestep,\n                              seed=None,\n                              device=\"cuda\"):\n\n    total_time = 5.0\n    dt = 0.01\n    n_timesteps_per_iter = 20\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # --- Physical domain ---\n    domain_x, domain_y, domain_t = (0.0, 2.5), (0.0, 1.0), (0.0, total_time)\n\n    # --- Scaling functions for [-1, 1] ---\n    def scale_x(x): return 2 * (x - domain_x[0]) / (domain_x[1] - domain_x[0]) - 1\n    def scale_y(y): return 2 * (y - domain_y[0]) / (domain_y[1] - domain_y[0]) - 1\n    def scale_t(t): return 2 * (t - domain_t[0]) / (domain_t[1] - domain_t[0]) - 1\n\n    # --- Scaled domain and regions ---\n    domain_x_scaled = (scale_x(domain_x[0]), scale_x(domain_x[1]))  # (-1, 1)\n    domain_y_scaled = (scale_y(domain_y[0]), scale_y(domain_y[1]))  # (-1, 1)\n    domain_t_scaled = (scale_t(domain_t[0]), scale_t(domain_t[1]))  # (-1, 1)\n\n    refine_1 = ((scale_x(0.35), scale_x(0.65)), (scale_y(0.35), scale_y(0.65)))\n    refine_2 = ((scale_x(0.65), scale_x(2.5)), (scale_y(0.35), scale_y(0.65)))\n\n    # --- Obstacle (scaled) ---\n    obstacle_center = (scale_x(0.5), scale_y(0.5))\n    obstacle_radius = 0.05\n    obstacle_radius_scaled = 2 * obstacle_radius / (domain_x[1] - domain_x[0])\n    obstacle_radius2 = obstacle_radius_scaled ** 2\n\n    # --- Region weights ---\n    w1, w2 = 0.40, 0.35\n    w3 = 1.0 - (w1 + w2)\n\n    timesteps = np.arange(0, total_time + dt, dt)\n    chosen_timesteps = np.random.choice(timesteps, size=n_timesteps_per_iter, replace=False)\n    chosen_timesteps_scaled = scale_t(chosen_timesteps)\n\n    # --- Vectorized sampling ---\n    total_pts = N_per_timestep * n_timesteps_per_iter\n    regions = np.random.choice(3, size=total_pts, p=[w1, w2, w3])\n\n    x = np.empty(total_pts, dtype=np.float32)\n    y = np.empty(total_pts, dtype=np.float32)\n    t = np.repeat(chosen_timesteps_scaled, N_per_timestep).astype(np.float32)\n\n    # Region 1\n    mask1 = regions == 0\n    n1 = mask1.sum()\n    x[mask1] = np.random.uniform(*refine_1[0], n1)\n    y[mask1] = np.random.uniform(*refine_1[1], n1)\n\n    # Region 2\n    mask2 = regions == 1\n    n2 = mask2.sum()\n    x[mask2] = np.random.uniform(*refine_2[0], n2)\n    y[mask2] = np.random.uniform(*refine_2[1], n2)\n\n    # Region 3 (rest of domain)\n    mask3 = regions == 2\n    n3 = mask3.sum()\n    x[mask3] = np.random.uniform(*domain_x_scaled, n3)\n    y[mask3] = np.random.uniform(*domain_y_scaled, n3)\n\n    # --- Remove obstacle points (vectorized) ---\n    dx = x - obstacle_center[0]\n    dy = y - obstacle_center[1]\n    mask_keep = dx**2 + dy**2 >= obstacle_radius2\n    x, y, t = x[mask_keep], y[mask_keep], t[mask_keep]\n\n    # --- Fast resampling if not enough points ---\n    n_needed = total_pts - len(x)\n    if n_needed > 0:\n        xr = np.random.uniform(*refine_2[0], n_needed)\n        yr = np.random.uniform(*refine_2[1], n_needed)\n        tr = np.random.choice(chosen_timesteps_scaled, size=n_needed)\n        dxr = xr - obstacle_center[0]\n        dyr = yr - obstacle_center[1]\n        keep = dxr**2 + dyr**2 >= obstacle_radius2\n        x = np.concatenate([x, xr[keep]])\n        y = np.concatenate([y, yr[keep]])\n        t = np.concatenate([t, tr[keep]])\n        x, y, t = x[:total_pts], y[:total_pts], t[:total_pts]\n\n    # Combine and shuffle\n    pts_np = np.stack([x, y, t], axis=1)\n    np.random.shuffle(pts_np)\n\n    return torch.tensor(pts_np, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.297602Z","iopub.execute_input":"2025-11-24T09:06:17.297797Z","iopub.status.idle":"2025-11-24T09:06:17.315489Z","shell.execute_reply.started":"2025-11-24T09:06:17.297783Z","shell.execute_reply":"2025-11-24T09:06:17.314834Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def sample_inlet(N_per_timestep,\n                 seed=None,\n                 device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # Original domain (before scaling)\n    x_in_original = 0.0\n    y_bounds_original = (0.0, 1.0)\n    t_bounds_original = (0.0, 5.0)\n    dt = 0.01\n\n    # --- Scale function from [min, max] → [-1, 1] ---\n    def scale(val, vmin, vmax):\n        return 2 * (val - vmin) / (vmax - vmin) - 1\n\n    # Scale boundaries\n    x_in = scale(x_in_original, 0.0, 2.5)       # if your domain in x is [0, 2.5]\n    y_min = scale(y_bounds_original[0], 0.0, 1.0)\n    y_max = scale(y_bounds_original[1], 0.0, 1.0)\n    t_min = scale(t_bounds_original[0], 0.0, 5.0)\n    t_max = scale(t_bounds_original[1], 0.0, 5.0)\n\n    timesteps = np.arange(t_min, t_max + (2 * (dt / (t_bounds_original[1] - t_bounds_original[0]))), \n                          2 * (dt / (t_bounds_original[1] - t_bounds_original[0])))\n\n    all_points = []\n\n    # Generate samples along inlet (x = x_in, y ∈ [-1, 1], t ∈ [-1, 1])\n    for t in timesteps:\n        y = np.random.uniform(y_min, y_max, N_per_timestep)\n        x = np.full(N_per_timestep, x_in)\n        t_vals = np.full(N_per_timestep, t)\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n    np.random.shuffle(all_points)\n\n    # Convert to torch\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.317555Z","iopub.execute_input":"2025-11-24T09:06:17.317748Z","iopub.status.idle":"2025-11-24T09:06:17.338786Z","shell.execute_reply.started":"2025-11-24T09:06:17.317734Z","shell.execute_reply":"2025-11-24T09:06:17.338165Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def sample_cylinder_surface(N_per_timestep,\n                            seed=None,\n                            device=\"cuda\"):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # Original domain and cylinder info\n    total_time = 5.0\n    dt = 0.01\n    cyl_center = (0.5, 0.5)\n    cyl_radius = 0.05\n\n    # Original domain bounds\n    x_bounds = (0.0, 2.5)\n    y_bounds = (0.0, 1.0)\n    t_bounds = (0.0, total_time)\n\n    # --- Scaling function [min, max] → [-1, 1] ---\n    def scale(val, vmin, vmax):\n        return 2 * (val - vmin) / (vmax - vmin) - 1\n\n    # Scale time range and compute scaled time step\n    t_min = scale(t_bounds[0], *t_bounds)\n    t_max = scale(t_bounds[1], *t_bounds)\n    dt_scaled = 2 * (dt / (t_bounds[1] - t_bounds[0]))  # scaled time step\n\n    timesteps = np.arange(t_min, t_max + dt_scaled, dt_scaled)\n    all_points = []\n\n    # Cylinder center and radius scaled to [-1, 1]\n    cx_scaled = scale(cyl_center[0], *x_bounds)\n    cy_scaled = scale(cyl_center[1], *y_bounds)\n    r_scaled_x = 2 * cyl_radius / (x_bounds[1] - x_bounds[0])\n    r_scaled_y = 2 * cyl_radius / (y_bounds[1] - y_bounds[0])\n\n    for t in timesteps:\n        theta = np.random.uniform(0, 2*np.pi, N_per_timestep)\n\n        # Scale radius independently in x and y directions\n        x = cx_scaled + r_scaled_x * np.cos(theta)\n        y = cy_scaled + r_scaled_y * np.sin(theta)\n        t_vals = np.full(N_per_timestep, t)\n\n        pts_np = np.stack([x, y, t_vals], axis=1)\n        all_points.append(pts_np)\n\n    all_points = np.vstack(all_points)\n    np.random.shuffle(all_points)\n\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.339389Z","iopub.execute_input":"2025-11-24T09:06:17.339587Z","iopub.status.idle":"2025-11-24T09:06:17.354781Z","shell.execute_reply.started":"2025-11-24T09:06:17.339573Z","shell.execute_reply":"2025-11-24T09:06:17.354048Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def sample_top_bottom(N_per_timestep,\n                      seed=None,\n                      device=\"cpu\"):\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    # --- Original physical domain ---\n    total_time = 5.0\n    dt = 0.01\n    x_bounds = (0.0, 2.5)\n    y_bounds = (0.0, 1.0)\n    t_bounds = (0.0, total_time)\n\n    y_top = y_bounds[1]\n    y_bot = y_bounds[0]\n\n    # --- Scaling function [min, max] → [-1, 1] ---\n    def scale(val, vmin, vmax):\n        return 2 * (val - vmin) / (vmax - vmin) - 1\n\n    # --- Pre-scale bounds and constants ---\n    x_min_scaled, x_max_scaled = scale(np.array(x_bounds), *x_bounds)\n    y_top_scaled = scale(y_top, *y_bounds)\n    y_bot_scaled = scale(y_bot, *y_bounds)\n    t_min_scaled, t_max_scaled = scale(np.array(t_bounds), *t_bounds)\n\n    # Compute scaled timestep spacing\n    dt_scaled = 2 * (dt / (t_bounds[1] - t_bounds[0]))\n\n    timesteps = np.arange(t_min_scaled, t_max_scaled + dt_scaled, dt_scaled)\n\n    # Split evenly between top and bottom\n    N_top = N_per_timestep // 2\n    N_bot = N_per_timestep - N_top\n\n    all_points = []\n\n    for t in timesteps:\n        # --- Top wall ---\n        x_top = np.random.uniform(x_min_scaled, x_max_scaled, N_top)\n        y_top_arr = np.full(N_top, y_top_scaled)\n        t_top = np.full(N_top, t)\n        pts_top = np.stack([x_top, y_top_arr, t_top], axis=1)\n\n        # --- Bottom wall ---\n        x_bot = np.random.uniform(x_min_scaled, x_max_scaled, N_bot)\n        y_bot_arr = np.full(N_bot, y_bot_scaled)\n        t_bot = np.full(N_bot, t)\n        pts_bot = np.stack([x_bot, y_bot_arr, t_bot], axis=1)\n\n        all_points.append(pts_top)\n        all_points.append(pts_bot)\n\n    all_points = np.vstack(all_points)\n    np.random.shuffle(all_points)\n\n    points = torch.tensor(all_points, dtype=torch.float32, device=device)\n    return points\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.355504Z","iopub.execute_input":"2025-11-24T09:06:17.355716Z","iopub.status.idle":"2025-11-24T09:06:17.374939Z","shell.execute_reply.started":"2025-11-24T09:06:17.355702Z","shell.execute_reply":"2025-11-24T09:06:17.374234Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import torch\n\ndef sample_initial(N,\n                   xyt_tensor_ic, \n                   uvp_tensor_ic,\n                   device=\"cuda\"):\n\n    # --- Original physical bounds ---\n    x_bounds = (0.0, 2.5)\n    y_bounds = (0.0, 1.0)\n\n    # --- Scaling function ---\n    def scale(val, vmin, vmax):\n        return 2 * (val - vmin) / (vmax - vmin) - 1\n\n    # --- Scaled geometry and regions ---\n    cyl_center = (scale(0.5, *x_bounds), scale(0.5, *y_bounds))\n    cyl_radius = 2 * (0.05 / (x_bounds[1] - x_bounds[0]))  # scale length (Δx=2.5)\n    \n    bound1 = (\n        (scale(0.35, *x_bounds), scale(0.65, *x_bounds)),\n        (scale(0.35, *y_bounds), scale(0.65, *y_bounds))\n    )\n\n    bound2 = (\n        (scale(0.65, *x_bounds), scale(2.5, *x_bounds)),\n        (scale(0.35, *y_bounds), scale(0.65, *y_bounds))\n    )\n\n    # --- Percentages ---\n    perc_r1, perc_r2, perc_r3 = 0.4, 0.35, 0.25\n    N_r1 = int(N * perc_r1)\n    N_r2 = int(N * perc_r2)\n    N_r3 = N - N_r1 - N_r2\n\n    # --- Coordinates ---\n    x = xyt_tensor_ic[:, 0]\n    y = xyt_tensor_ic[:, 1]\n\n    # --- Mask out cylinder ---\n    cx, cy = cyl_center\n    dx, dy = x - cx, y - cy\n    mask_cyl = (dx**2 + dy**2) >= cyl_radius**2\n    xyt_valid = xyt_tensor_ic[mask_cyl]\n    uvp_valid = uvp_tensor_ic[mask_cyl]\n\n    # --- Region 1 (around cylinder) ---\n    mask_r1 = ((xyt_valid[:, 0] >= bound1[0][0]) & (xyt_valid[:, 0] <= bound1[0][1]) &\n               (xyt_valid[:, 1] >= bound1[1][0]) & (xyt_valid[:, 1] <= bound1[1][1]))\n\n    # --- Region 2 (wake) ---\n    mask_r2 = ((xyt_valid[:, 0] >= bound2[0][0]) & (xyt_valid[:, 0] <= bound2[0][1]) &\n               (xyt_valid[:, 1] >= bound2[1][0]) & (xyt_valid[:, 1] <= bound2[1][1]))\n\n    # --- Region 3 = rest ---\n    mask_r3 = ~(mask_r1 | mask_r2)\n\n    # --- Sampling helper ---\n    def sample_region(mask, N):\n        xyt_reg = xyt_valid[mask]\n        uvp_reg = uvp_valid[mask]\n        if len(xyt_reg) == 0:\n            return None, None\n        replace = xyt_reg.shape[0] < N\n        idx = torch.randint(0, xyt_reg.shape[0], (N,), device=\"cpu\", dtype=torch.long) if replace else \\\n              torch.randperm(xyt_reg.shape[0])[:N]\n        return xyt_reg[idx].to(device), uvp_reg[idx].to(device)\n\n    # --- Sample each region ---\n    xyt_r1, uvp_r1 = sample_region(mask_r1, N_r1)\n    xyt_r2, uvp_r2 = sample_region(mask_r2, N_r2)\n    xyt_r3, uvp_r3 = sample_region(mask_r3, N_r3)\n\n    # --- Concatenate ---\n    sampled_xyt = torch.cat([t for t in [xyt_r1, xyt_r2, xyt_r3] if t is not None], dim=0)\n    sampled_uvp = torch.cat([t for t in [uvp_r1, uvp_r2, uvp_r3] if t is not None], dim=0)\n\n    return sampled_xyt, sampled_uvp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.375801Z","iopub.execute_input":"2025-11-24T09:06:17.376154Z","iopub.status.idle":"2025-11-24T09:06:17.399823Z","shell.execute_reply.started":"2025-11-24T09:06:17.376134Z","shell.execute_reply":"2025-11-24T09:06:17.399246Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import torch\n\ndef compute_residuals(model, X, sigma_k=1.0, sigma_e=1.3,\n                      C1=1.42, C2=1.68, rho=1.0):\n    \"\"\"\n    Compute PDE residuals for scaled coordinates (x', y', t') ∈ [-1, 1].\n    Handles: continuity, momentum, k, ε equations (RNG k–ε model).\n    \"\"\"\n\n    # --- Physical constants ---\n    nu = 5e-4  \n    \n    # --- Scaling factors (for chain rule) ---\n    Lx, Ly, Lt = 2.5, 1.0, 5.0\n    sx = 2.0 / Lx   # 0.8\n    sy = 2.0 / Ly   # 2.0\n    st = 2.0 / Lt   # 0.4\n\n    # --- Forward pass ---\n    X = X.clone().detach().requires_grad_(True)\n    ψ, p, k, eps = model(X)\n\n    # --- Velocity components from ψ ---\n    grads_ψ = torch.autograd.grad(ψ, X, grad_outputs=torch.ones_like(ψ),\n                                  create_graph=True, retain_graph=True)[0]\n    ψ_xp = grads_ψ[:, 0:1]\n    ψ_yp = grads_ψ[:, 1:2]\n    # Convert to physical derivatives\n    ψ_x = ψ_xp * sx\n    ψ_y = ψ_yp * sy\n\n    u = ψ_y\n    v = -ψ_x\n\n    # --- Velocity gradients ---\n    grads_u = torch.autograd.grad(u, X, grad_outputs=torch.ones_like(u),\n                                  create_graph=True, retain_graph=True)[0]\n    grads_v = torch.autograd.grad(v, X, grad_outputs=torch.ones_like(v),\n                                  create_graph=True, retain_graph=True)[0]\n\n    u_x = grads_u[:, 0:1] * sx\n    u_y = grads_u[:, 1:2] * sy\n    u_t = grads_u[:, 2:3] * st\n\n    v_x = grads_v[:, 0:1] * sx\n    v_y = grads_v[:, 1:2] * sy\n    v_t = grads_v[:, 2:3] * st\n\n    # --- Second derivatives ---\n    u_xx = torch.autograd.grad(u_x, X, torch.ones_like(u_x), create_graph=True, retain_graph=True)[0][:, 0:1] * sx**2\n    u_yy = torch.autograd.grad(u_y, X, torch.ones_like(u_y), create_graph=True, retain_graph=True)[0][:, 1:2] * sy**2\n    v_xx = torch.autograd.grad(v_x, X, torch.ones_like(v_x), create_graph=True, retain_graph=True)[0][:, 0:1] * sx**2\n    v_yy = torch.autograd.grad(v_y, X, torch.ones_like(v_y), create_graph=True, retain_graph=True)[0][:, 1:2] * sy**2\n\n    # --- Pressure gradients ---\n    grads_p = torch.autograd.grad(p, X, grad_outputs=torch.ones_like(p),\n                                  create_graph=True, retain_graph=True)[0]\n    p_x = grads_p[:, 0:1] * sx\n    p_y = grads_p[:, 1:2] * sy\n\n    # --- Continuity ---\n    cont = u_x + v_y\n\n    # --- Turbulent viscosity ---\n    nu_t = k**2 / (eps + 1e-8)\n    nu_eff = nu + nu_t\n\n    # --- Momentum residuals ---\n    mom_u = u_t + (u * u_x + v * u_y) + (1 / rho) * p_x - nu_eff * (u_xx + u_yy)\n    mom_v = v_t + (u * v_x + v * v_y) + (1 / rho) * p_y - nu_eff * (v_xx + v_yy)\n\n    # --- k-equation residual ---\n    grads_k = torch.autograd.grad(k, X, grad_outputs=torch.ones_like(k),\n                                  create_graph=True, retain_graph=True)[0]\n    k_x = grads_k[:, 0:1] * sx\n    k_y = grads_k[:, 1:2] * sy\n    k_t = grads_k[:, 2:3] * st\n    k_xx = torch.autograd.grad(k_x, X, torch.ones_like(k_x), create_graph=True, retain_graph=True)[0][:, 0:1] * sx**2\n    k_yy = torch.autograd.grad(k_y, X, torch.ones_like(k_y), create_graph=True, retain_graph=True)[0][:, 1:2] * sy**2\n\n    # Production term Pk\n    Sxx = u_x\n    Syy = v_y\n    Sxy = 0.5 * (u_y + v_x)\n    Pk = 2 * nu_t * (Sxx**2 + 2*Sxy**2 + Syy**2)\n\n    res_k = (\n        k_t\n        + (u * k_x + v * k_y)\n        - Pk\n        + eps\n        - (nu + nu_t / sigma_k) * (k_xx + k_yy)\n    )\n\n    # --- ε-equation residual ---\n    grads_eps = torch.autograd.grad(eps, X, grad_outputs=torch.ones_like(eps),\n                                    create_graph=True, retain_graph=True)[0]\n    e_x = grads_eps[:, 0:1] * sx\n    e_y = grads_eps[:, 1:2] * sy\n    e_t = grads_eps[:, 2:3] * st\n    e_xx = torch.autograd.grad(e_x, X, torch.ones_like(e_x), create_graph=True, retain_graph=True)[0][:, 0:1] * sx**2\n    e_yy = torch.autograd.grad(e_y, X, torch.ones_like(e_y), create_graph=True, retain_graph=True)[0][:, 1:2] * sy**2\n\n    R = torch.zeros_like(k)  # RNG correction term placeholder\n    res_eps = (\n        e_t\n        + (u * e_x + v * e_y)\n        - (C1 - R) * (eps / k) * Pk\n        + C2 * eps**2 / k\n        - (nu + nu_t / sigma_e) * (e_xx + e_yy)\n    )\n\n    return cont, mom_u, mom_v, res_k, res_eps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.400478Z","iopub.execute_input":"2025-11-24T09:06:17.400716Z","iopub.status.idle":"2025-11-24T09:06:17.419712Z","shell.execute_reply.started":"2025-11-24T09:06:17.400701Z","shell.execute_reply":"2025-11-24T09:06:17.419109Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def data_loss(model, pts, u, v, p, device):\n\n    # Model prediction\n    u_pred, v_pred, p_pred = model.velocity_pressure(pts)\n\n    mse = nn.MSELoss()\n    return mse(u_pred, u) + mse(v_pred, v) + mse(p_pred, p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.420424Z","iopub.execute_input":"2025-11-24T09:06:17.420673Z","iopub.status.idle":"2025-11-24T09:06:17.441015Z","shell.execute_reply.started":"2025-11-24T09:06:17.420653Z","shell.execute_reply":"2025-11-24T09:06:17.440315Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"os.makedirs('models', exist_ok=True)\nNf, Nf_data, Nic, Nd, Nn, Ni = 40000, 10000, 50000, 60, 700, 150","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.441608Z","iopub.execute_input":"2025-11-24T09:06:17.441926Z","iopub.status.idle":"2025-11-24T09:06:17.458716Z","shell.execute_reply.started":"2025-11-24T09:06:17.441911Z","shell.execute_reply":"2025-11-24T09:06:17.458032Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def compute_residuals_in_batches(model, X, batch_size=10000):\n\n    cont_list, mu_list, mv_list, rk_list, re_list = [], [], [], [], []\n    n = X.size(0)\n\n    for i in range(0, n, batch_size):\n        X_batch = X[i:i+batch_size].clone().detach().requires_grad_(True)\n        \n        # Compute all residuals for this batch\n        cont, mom_u, mom_v, res_k, res_eps = compute_residuals(model, X_batch)\n\n        # Store each batch result\n        cont_list.append(cont.detach())\n        mu_list.append(mom_u.detach())\n        mv_list.append(mom_v.detach())\n        rk_list.append(res_k.detach())\n        re_list.append(res_eps.detach())\n\n        # Free up memory\n        del cont, mom_u, mom_v, res_k, res_eps, X_batch\n        torch.cuda.empty_cache()\n\n    # Concatenate results back into full tensors\n    return (\n        torch.cat(cont_list, dim=0),\n        torch.cat(mu_list, dim=0),\n        torch.cat(mv_list, dim=0),\n        torch.cat(rk_list, dim=0),\n        torch.cat(re_list, dim=0)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.459352Z","iopub.execute_input":"2025-11-24T09:06:17.459636Z","iopub.status.idle":"2025-11-24T09:06:17.475642Z","shell.execute_reply.started":"2025-11-24T09:06:17.459621Z","shell.execute_reply":"2025-11-24T09:06:17.474987Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\nfrom torch.cuda.amp import autocast, GradScaler\n\n\nmse_loss = nn.MSELoss()\n\ndef train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=40000, print_every=2000, save_every=2000,\n               λ_data=1.0, alpha_pde=1.0, seed=None, device=None,\n               checkpoint_dir=\"models\"):\n    \n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Training on device: {device}\")\n    print(\"Start training loop\")\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=100,      # restart every 100 iterations\n        T_mult=1,     # keep restart period constant\n        eta_min=1e-6  # minimum LR between restarts\n    )\n    scaler = GradScaler()\n\n    start_iter = 0\n\n    # --- Try to resume from the latest checkpoint ---\n    latest_ckpt = None\n    if os.path.exists(checkpoint_dir):\n        ckpts = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"pinn_checkpoint_\")]\n        if ckpts:\n            latest_ckpt = max(ckpts, key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0]))\n\n    if latest_ckpt:\n        path = os.path.join(checkpoint_dir, latest_ckpt)\n        print(f\"Loading checkpoint from {path}...\")\n        checkpoint = torch.load(path, map_location=device)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        start_iter = checkpoint[\"iter\"] + 1\n        print(f\"Resuming training from iteration {start_iter}\")\n\n\n    for it in range(start_iter, num_iters):\n        optimizer.zero_grad()\n\n        with autocast():\n\n            # Physics residual loss\n            X_f = sample_collocation_points(N_per_timestep=Nf,\n                                              seed=None,\n                                              device=\"cuda\")\n            \n            X_f_data, u, v, p = sample_points(inputs=xyt_tensor,\n                                              outputs=uvp_tensor,\n                                              N_per_timestep=Nf_data,\n                                              seed=None,\n                                              device=\"cuda\")\n        \n    \n    \n                        # Compute residuals in batches\n            cont, mom_u, mom_v, res_k, res_e = compute_residuals_in_batches(model, X_f, batch_size=10000)\n            \n            # PDE loss: continuity, momentum (u,v), turbulence (k, ε)\n            loss_f = (\n                mse_loss(cont, torch.zeros_like(cont)) +\n                mse_loss(mom_u, torch.zeros_like(mom_u)) +\n                mse_loss(mom_v, torch.zeros_like(mom_v)) +\n                mse_loss(res_k, torch.zeros_like(res_k)) +\n                mse_loss(res_e, torch.zeros_like(res_e))\n            )\n    \n    \n            # Data loss\n            loss_data = data_loss(model, X_f_data, u, v, p, device=\"cuda\")\n    \n    \n            # Initial condition\n            X_ic, Y_ic = sample_initial(N=Nic,\n                                        xyt_tensor_ic=xyt_tensor_ic,\n                                        uvp_tensor_ic=uvp_tensor_ic, \n                                        device=\"cuda\")\n    \n            \n            u_ic, v_ic, p_ic = model.velocity_pressure(X_ic)\n            loss_ic = (mse_loss(u_ic, Y_ic[:, 0:1]) + \n                       mse_loss(v_ic, Y_ic[:, 1:2]) + \n                       mse_loss(p_ic, Y_ic[:, 2:3]))\n    \n    \n            # Inlet condition\n            X_in = sample_inlet(N_per_timestep=Ni,\n                                seed=None, \n                                device=\"cuda\")\n    \n            \n            u_in, v_in, p_in = model.velocity_pressure(X_in)\n            loss_in = (mse_loss(u_in, torch.ones_like(u_in)) + \n                       mse_loss(v_in, torch.zeros_like(v_in))\n                      )\n    \n    \n            # Cylinder surface\n            X_cyl = sample_cylinder_surface(N_per_timestep=Nd,\n                                            seed=None, \n                                            device=\"cuda\")\n    \n            \n            u_cyl, v_cyl, p_cyl = model.velocity_pressure(X_cyl)\n\n            loss_cyl = (\n                mse_loss(u_cyl, torch.zeros_like(u_cyl)) +  # u = 0\n                mse_loss(v_cyl, torch.zeros_like(v_cyl))    # v = 0\n            )\n        \n    \n            # Top/bottom boundaries\n            X_tb = sample_top_bottom(N_per_timestep=Nn, \n                                     seed=None, \n                                     device=\"cuda\")\n    \n            u_tb, v_tb, p_tb = model.velocity_pressure(X_tb)\n\n            loss_tb = (\n                mse_loss(v_tb, torch.zeros_like(v_tb))      # v = 0 (no vertical velocity)\n            )\n                \n    \n            # Total loss\n            loss = alpha_pde * loss_f + λ_data * loss_data + loss_ic + loss_in + loss_cyl + loss_tb\n\n\n        \n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step(it + 1)\n\n        torch.cuda.empty_cache()\n\n\n        if it % print_every == 0:\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            print(f\"Adam Iter {it}/{num_iters} - LR {current_lr:.2e} - Total loss {loss.item():.6f}\\n\"\n                  f\"Data {loss_data.item():.6f} | PDE {loss_f.item():.4e}\\n\"\n                  f\"IC {loss_ic.item():.4e}, Inlet {loss_in.item():.4e}\\n\"\n                  f\"Cylinder {loss_cyl.item():.4e}, TopBottom {loss_tb.item():.4e}\")\n\n         # --- Save checkpoint every save_every iterations ---\n        if (it + 1) % save_every == 0:\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            ckpt_path = os.path.join(checkpoint_dir, f\"resnet_pinn_checkpoint_{it+1}.pth\")\n            torch.save({\n                \"iter\": it,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scaler_state\": scaler.state_dict()\n            }, ckpt_path)\n            print(f\"Checkpoint saved → {ckpt_path}\")\n\n         \n            \n    current_lr = optimizer.param_groups[0][\"lr\"]\n    print(f\"Adam Iter {num_iters-1} - LR {current_lr:.2e} - Total loss {loss.item():.6f}\\n\"\n              f\"Data {loss_data.item():.6f} | PDE {loss_f.item():.4e}\\n\"\n              f\"IC {loss_ic.item():.4e}, Inlet {loss_in.item():.4e}\\n\"\n              f\"Cylinder {loss_cyl.item():.4e}, TopBottom {loss_tb.item():.4e}\")\n\n\n    # Final model save\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    final_path = os.path.join(checkpoint_dir, \"resnet_pinn_ns_adam_final.pth\")\n    torch.save(model.state_dict(), final_path)\n    print(f\"Final model saved at {final_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.476297Z","iopub.execute_input":"2025-11-24T09:06:17.476588Z","iopub.status.idle":"2025-11-24T09:06:17.504587Z","shell.execute_reply.started":"2025-11-24T09:06:17.476565Z","shell.execute_reply":"2025-11-24T09:06:17.504047Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PINN().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:06:17.505236Z","iopub.execute_input":"2025-11-24T09:06:17.505413Z","iopub.status.idle":"2025-11-24T09:06:17.527898Z","shell.execute_reply.started":"2025-11-24T09:06:17.505400Z","shell.execute_reply":"2025-11-24T09:06:17.527399Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"train_adam(model, Nf, Nf_data, Nic, Nd, Nn, Ni,\n               num_iters=1000, print_every=10, save_every=100,\n               λ_data=1.0, alpha_pde=1e-3, seed=None, device=device,\n               checkpoint_dir=\"models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:15:04.906656Z","iopub.execute_input":"2025-11-24T09:15:04.907281Z","execution_failed":"2025-11-24T14:26:44.842Z"}},"outputs":[{"name":"stdout","text":"Training on device: cuda\nStart training loop\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_39/2235816808.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_39/2235816808.py:51: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Adam Iter 0/1000 - LR 1.00e-04 - Total loss 1.868529\nData 0.526898 | PDE 2.2048e+02\nIC 2.2028e-01, Inlet 1.3527e-01\nCylinder 7.5164e-01, TopBottom 1.3969e-02\nAdam Iter 10/1000 - LR 9.71e-05 - Total loss 1.439418\nData 0.341254 | PDE 1.2499e+02\nIC 2.1553e-01, Inlet 1.6030e-01\nCylinder 5.8882e-01, TopBottom 8.5301e-03\nAdam Iter 20/1000 - LR 8.96e-05 - Total loss 1.262884\nData 0.256053 | PDE 9.7293e+01\nIC 1.9793e-01, Inlet 1.6104e-01\nCylinder 5.4468e-01, TopBottom 5.8786e-03\nAdam Iter 30/1000 - LR 7.83e-05 - Total loss 1.091079\nData 0.159499 | PDE 7.0977e+01\nIC 1.8630e-01, Inlet 1.5932e-01\nCylinder 5.1004e-01, TopBottom 4.9446e-03\nAdam Iter 40/1000 - LR 6.43e-05 - Total loss 1.016320\nData 0.125907 | PDE 7.2354e+01\nIC 1.8008e-01, Inlet 1.5795e-01\nCylinder 4.7457e-01, TopBottom 5.4646e-03\nAdam Iter 50/1000 - LR 4.89e-05 - Total loss 0.945138\nData 0.108874 | PDE 5.1142e+01\nIC 1.7744e-01, Inlet 1.5855e-01\nCylinder 4.4179e-01, TopBottom 7.3503e-03\nAdam Iter 60/1000 - LR 3.37e-05 - Total loss 0.907471\nData 0.094722 | PDE 5.5287e+01\nIC 1.7830e-01, Inlet 1.5528e-01\nCylinder 4.1414e-01, TopBottom 9.7461e-03\nAdam Iter 70/1000 - LR 2.02e-05 - Total loss 0.894777\nData 0.092513 | PDE 6.7311e+01\nIC 1.8070e-01, Inlet 1.5588e-01\nCylinder 3.8749e-01, TopBottom 1.0880e-02\nAdam Iter 80/1000 - LR 9.56e-06 - Total loss 0.886432\nData 0.092096 | PDE 7.0892e+01\nIC 1.7877e-01, Inlet 1.5121e-01\nCylinder 3.8260e-01, TopBottom 1.0869e-02\nAdam Iter 90/1000 - LR 2.97e-06 - Total loss 0.901476\nData 0.099999 | PDE 8.5465e+01\nIC 1.7717e-01, Inlet 1.5086e-01\nCylinder 3.7709e-01, TopBottom 1.0896e-02\nCheckpoint saved → models/resnet_pinn_checkpoint_100.pth\nAdam Iter 100/1000 - LR 1.00e-04 - Total loss 0.890210\nData 0.088548 | PDE 8.5581e+01\nIC 1.7682e-01, Inlet 1.4887e-01\nCylinder 3.7952e-01, TopBottom 1.0867e-02\nAdam Iter 110/1000 - LR 9.71e-05 - Total loss 1.045440\nData 0.093803 | PDE 3.1129e+02\nIC 1.6254e-01, Inlet 1.1922e-01\nCylinder 3.4796e-01, TopBottom 1.0625e-02\nAdam Iter 120/1000 - LR 8.96e-05 - Total loss 1.877833\nData 0.096913 | PDE 1.2124e+03\nIC 1.7468e-01, Inlet 1.1153e-01\nCylinder 2.6992e-01, TopBottom 1.2427e-02\nAdam Iter 130/1000 - LR 7.83e-05 - Total loss 3.540198\nData 0.089528 | PDE 2.9272e+03\nIC 1.5743e-01, Inlet 8.0874e-02\nCylinder 2.7212e-01, TopBottom 1.3016e-02\nAdam Iter 140/1000 - LR 6.43e-05 - Total loss 4.864398\nData 0.103566 | PDE 4.2749e+03\nIC 1.7877e-01, Inlet 9.7916e-02\nCylinder 1.9592e-01, TopBottom 1.3362e-02\nAdam Iter 150/1000 - LR 4.89e-05 - Total loss 5.925823\nData 0.095440 | PDE 5.3561e+03\nIC 1.6503e-01, Inlet 8.7732e-02\nCylinder 2.0824e-01, TopBottom 1.3257e-02\nAdam Iter 160/1000 - LR 3.37e-05 - Total loss 5.917879\nData 0.101562 | PDE 5.3509e+03\nIC 1.7132e-01, Inlet 9.6408e-02\nCylinder 1.8456e-01, TopBottom 1.3099e-02\nAdam Iter 170/1000 - LR 2.02e-05 - Total loss 6.152700\nData 0.102975 | PDE 5.5909e+03\nIC 1.5972e-01, Inlet 8.9500e-02\nCylinder 1.9761e-01, TopBottom 1.1977e-02\nAdam Iter 180/1000 - LR 9.56e-06 - Total loss 6.328274\nData 0.104409 | PDE 5.7738e+03\nIC 1.6053e-01, Inlet 9.4331e-02\nCylinder 1.8382e-01, TopBottom 1.1407e-02\nAdam Iter 190/1000 - LR 2.97e-06 - Total loss 6.208409\nData 0.106511 | PDE 5.6489e+03\nIC 1.6684e-01, Inlet 9.8207e-02\nCylinder 1.7656e-01, TopBottom 1.1415e-02\nCheckpoint saved → models/resnet_pinn_checkpoint_200.pth\nAdam Iter 200/1000 - LR 1.00e-04 - Total loss 5.907955\nData 0.095599 | PDE 5.3626e+03\nIC 1.6294e-01, Inlet 9.5803e-02\nCylinder 1.7974e-01, TopBottom 1.1244e-02\nAdam Iter 210/1000 - LR 9.71e-05 - Total loss 6.147470\nData 0.130399 | PDE 5.5876e+03\nIC 1.5400e-01, Inlet 9.9230e-02\nCylinder 1.6695e-01, TopBottom 9.2822e-03\nAdam Iter 220/1000 - LR 8.96e-05 - Total loss 5.956939\nData 0.105499 | PDE 5.4283e+03\nIC 1.3693e-01, Inlet 8.9493e-02\nCylinder 1.8761e-01, TopBottom 9.0732e-03\nAdam Iter 230/1000 - LR 7.83e-05 - Total loss 5.686458\nData 0.108060 | PDE 5.1691e+03\nIC 1.3915e-01, Inlet 1.0286e-01\nCylinder 1.5815e-01, TopBottom 9.1001e-03\nAdam Iter 240/1000 - LR 6.43e-05 - Total loss 5.190907\nData 0.110806 | PDE 4.6693e+03\nIC 1.3577e-01, Inlet 9.4992e-02\nCylinder 1.7211e-01, TopBottom 7.9763e-03\nAdam Iter 250/1000 - LR 4.89e-05 - Total loss 5.737386\nData 0.095713 | PDE 5.2360e+03\nIC 1.3219e-01, Inlet 9.2144e-02\nCylinder 1.7357e-01, TopBottom 7.7236e-03\n","output_type":"stream"}],"execution_count":null}]}